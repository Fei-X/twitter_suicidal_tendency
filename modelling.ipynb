{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to take note before using this modelling notebook\n",
    "\n",
    "1. There are 2 approaches to using the text corpus  \n",
    "    a. LDA: Hard to explain because clusters are not labelled but dimensionality has been reduced to 5 (based on grid)  \n",
    "    b. TFIDF: 173 (based on vectorizer) tfidf float numbers exist per tweet. Easier explanability but high dimensionality)  \n",
    "2. A new feature \"day_after\" has been added. Remember to include it in the modelling step if you wish to.  \n",
    "3. Remember to do scaling on numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flyxs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\users\\flyxs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))\n",
    "\n",
    "# Remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tf vectorizer\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Adding a list of stop words to the wordlist\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(['suicidal'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=0.01, stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -234533.78816095446\n",
      "Perplexity:  146.79236506626245\n"
     ]
    }
   ],
   "source": [
    "# Use vectors for LDA\n",
    "\n",
    "x = count_vectorizer.fit_transform(df['tweet'])\n",
    "lda_model = LatentDirichletAllocation(n_components=5, learning_decay=0.7, random_state=2021)\n",
    "xtr = lda_model.fit_transform(x)\n",
    "\n",
    "# Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(x))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(x))\n",
    "\n",
    "data_x = pd.DataFrame(xtr).add_prefix('topic_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing ,model_selection\n",
    "\n",
    "# Take log for some features and generate data_x\n",
    "features_to_log = ['nlikes', 'nreplies', 'nretweets', 'tweets', 'following', 'followers', 'likes', 'media']\n",
    "for feature in features_to_log:\n",
    "    data_x[feature] = np.log1p(df[feature])\n",
    "    \n",
    "# 'day' take absolute values.\n",
    "df['day'] = (df['day'] - 4).abs()\n",
    "\n",
    "# Adding features\n",
    "features_to_add = ['day','reply_to', 'url','join_time', 'day_after','tweet_length', 'tweet_sentiment', 'bio_sentiment', 'first_person', 'second_person', 'third_person']\n",
    "for feature in features_to_add:\n",
    "    data_x[feature] = df[feature]\n",
    "\n",
    "    \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "data_x.iloc[:, 5:] = scaler.fit_transform(data_x.iloc[:, 5:])\n",
    "data_y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data_x, data_y, test_size = 0.2, random_state = 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7221\n",
       "1    2017\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things to take note before using this modelling notebook\n",
    "\n",
    "1. There are 2 approaches to using the text corpus\n",
    "    a. LDA: Hard to explain because clusters are not labelled but dimensionality has been reduced to 5 (based on grid)\n",
    "    b. TFIDF: 173 (based on vectorizer) tfidf float numbers exist per tweet. Easier explanability but high dimensionality)\n",
    "2. A new feature \"day_after\" has been added. Remember to include it in the modelling step if you wish to.\n",
    "3. Remember to do scaling on numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girls i like e...\n",
       "2    content warnings for billies documentary  stro...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pays principle is a threat to thi...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girl i like em...\n",
       "2    content warning for billy documentary  strong ...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pay principle is a threat to this...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet              object\n",
       "label               int64\n",
       "day                 int64\n",
       "nlikes              int64\n",
       "nreplies            int64\n",
       "nretweets           int64\n",
       "reply_to            int64\n",
       "url                 int64\n",
       "join_time           int64\n",
       "tweets              int64\n",
       "following           int64\n",
       "followers           int64\n",
       "likes               int64\n",
       "media               int64\n",
       "day_after           int64\n",
       "tweet_length        int64\n",
       "tweet_sentiment     int64\n",
       "bio_sentiment       int64\n",
       "first_person        int64\n",
       "second_person       int64\n",
       "third_person        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = df['label']\n",
    "data_x = df['tweet']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, stratify=data_y, random_state = 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run keras Tokenizer\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train_tok = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('LSTM_models/tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training constants\n",
    "MAX_SEQ_LEN = 53 # Based on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vec = pad_sequences(x_train_tok, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(x_test_tok, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 14216\n",
      "Max Token Index: 14216 \n",
      "\n",
      "Sample Tweet Before Processing: we can at least prevent people from such suicidal act  using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda\n",
      "Sample Tweet After Processing: ['we can at least prevent people from such suicidal act using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 45, 43, 358, 788, 27, 42, 296, 1, 372, 470, 2691, 2146, 79, 768, 1153, 4575, 1957, 4576, 2691, 4577, 1958, 3629, 4578, 1957, 4579, 4580]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', x_train.values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_label = encoder.fit_transform(y_train.values)\n",
    "y_train_label = to_categorical(y_train_label) \n",
    "\n",
    "y_test_label = encoder.fit_transform(y_test.values)\n",
    "y_test_label = to_categorical(y_test_label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get class weights for the training data, this will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({0: 5776, 1: 1614})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ctr = Counter(y_train.values)\n",
    "print('Distribution of Classes:', ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6397160664819944, 1: 2.2893432465923174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OVO\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train_label,axis=1)\n",
    "cws_raw = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "label = [0,1]\n",
    "\n",
    "cws = dict(zip(label, cws_raw))\n",
    "\n",
    "print(cws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 53s 414ms/step - loss: 0.6398 - accuracy: 0.5507 - val_loss: 0.4279 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.56624, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 53s 461ms/step - loss: 0.3855 - accuracy: 0.8225 - val_loss: 0.4150 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00002: loss improved from 0.56624 to 0.38841, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 53s 457ms/step - loss: 0.2898 - accuracy: 0.8736 - val_loss: 0.4010 - val_accuracy: 0.8144\n",
      "\n",
      "Epoch 00003: loss improved from 0.38841 to 0.29063, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 53s 457ms/step - loss: 0.2130 - accuracy: 0.9054 - val_loss: 0.4754 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00004: loss improved from 0.29063 to 0.20948, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 53s 453ms/step - loss: 0.1578 - accuracy: 0.9337 - val_loss: 0.5349 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00005: loss improved from 0.20948 to 0.16417, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 55s 477ms/step - loss: 0.1180 - accuracy: 0.9530 - val_loss: 0.7447 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00006: loss improved from 0.16417 to 0.11353, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 54s 468ms/step - loss: 0.0835 - accuracy: 0.9667 - val_loss: 0.6751 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00007: loss improved from 0.11353 to 0.08735, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 53s 460ms/step - loss: 0.0854 - accuracy: 0.9649 - val_loss: 0.7985 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00008: loss improved from 0.08735 to 0.08411, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0579 - accuracy: 0.9766 - val_loss: 0.8978 - val_accuracy: 0.8074\n",
      "\n",
      "Epoch 00009: loss improved from 0.08411 to 0.05828, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 56s 487ms/step - loss: 0.0606 - accuracy: 0.9773 - val_loss: 0.8615 - val_accuracy: 0.7652\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.05828\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 63s 543ms/step - loss: 0.0641 - accuracy: 0.9728 - val_loss: 1.0328 - val_accuracy: 0.7825\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.05828\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 61s 527ms/step - loss: 0.0997 - accuracy: 0.9550 - val_loss: 1.0121 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.05828\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 61s 525ms/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 0.9319 - val_accuracy: 0.8144\n",
      "\n",
      "Epoch 00013: loss improved from 0.05828 to 0.05281, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 62s 531ms/step - loss: 0.0474 - accuracy: 0.9808 - val_loss: 1.0715 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00014: loss improved from 0.05281 to 0.04674, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 61s 526ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 1.1283 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00015: loss improved from 0.04674 to 0.03202, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 61s 523ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 1.2691 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.03202\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 62s 534ms/step - loss: 0.0260 - accuracy: 0.9898 - val_loss: 1.0839 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.03202\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 62s 531ms/step - loss: 0.0282 - accuracy: 0.9876 - val_loss: 1.1780 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00018: loss improved from 0.03202 to 0.02732, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 60s 513ms/step - loss: 0.0218 - accuracy: 0.9912 - val_loss: 1.2649 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00019: loss improved from 0.02732 to 0.02725, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 1.1455 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02725\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 62s 532ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 1.0295 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00021: loss improved from 0.02725 to 0.02633, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 60s 516ms/step - loss: 0.0327 - accuracy: 0.9862 - val_loss: 1.1965 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.02633\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0187 - accuracy: 0.9926 - val_loss: 1.2177 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00023: loss improved from 0.02633 to 0.02122, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 60s 520ms/step - loss: 0.0216 - accuracy: 0.9895 - val_loss: 1.2608 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.02122\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 59s 510ms/step - loss: 0.0197 - accuracy: 0.9920 - val_loss: 1.3434 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00025: loss improved from 0.02122 to 0.02114, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 60s 515ms/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 1.3724 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00026: loss improved from 0.02114 to 0.01651, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.0195 - accuracy: 0.9937 - val_loss: 1.3489 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01651\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 59s 510ms/step - loss: 0.0148 - accuracy: 0.9934 - val_loss: 1.4957 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01651\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0155 - accuracy: 0.9923 - val_loss: 1.3453 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00029: loss improved from 0.01651 to 0.01641, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 61s 530ms/step - loss: 0.0263 - accuracy: 0.9894 - val_loss: 1.2504 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01641\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0244 - accuracy: 0.9919 - val_loss: 0.9660 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01641\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 66s 571ms/step - loss: 0.0299 - accuracy: 0.9883 - val_loss: 1.1433 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01641\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 65s 560ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 1.2264 - val_accuracy: 0.7982\n",
      "\n",
      "Epoch 00033: loss improved from 0.01641 to 0.01535, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 60s 515ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 1.2998 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00034: loss improved from 0.01535 to 0.01272, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 57s 494ms/step - loss: 0.0108 - accuracy: 0.9951 - val_loss: 1.4616 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00035: loss improved from 0.01272 to 0.01192, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0361 - accuracy: 0.9863 - val_loss: 1.1838 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01192\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 59s 505ms/step - loss: 0.0175 - accuracy: 0.9913 - val_loss: 1.2356 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01192\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 58s 501ms/step - loss: 0.0122 - accuracy: 0.9948 - val_loss: 1.2768 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.01192\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0408 - accuracy: 0.9842 - val_loss: 0.9510 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.01192\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 62s 534ms/step - loss: 0.0234 - accuracy: 0.9894 - val_loss: 1.3178 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01192\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 58s 499ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 1.2172 - val_accuracy: 0.7781\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.01192\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 1.2588 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.01192\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 1.3279 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00043: loss improved from 0.01192 to 0.01080, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 59s 509ms/step - loss: 0.0090 - accuracy: 0.9964 - val_loss: 1.4419 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00044: loss improved from 0.01080 to 0.00960, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 59s 508ms/step - loss: 0.0091 - accuracy: 0.9958 - val_loss: 1.4182 - val_accuracy: 0.7890\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00960\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 59s 513ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 1.4443 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00960\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 59s 513ms/step - loss: 0.0149 - accuracy: 0.9936 - val_loss: 1.3381 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00960\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 59s 512ms/step - loss: 0.0106 - accuracy: 0.9958 - val_loss: 1.2640 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00960\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 58s 499ms/step - loss: 0.0134 - accuracy: 0.9934 - val_loss: 1.3064 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00960\n",
      "Epoch 50/100\n",
      "116/116 [==============================] - 63s 547ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 1.4729 - val_accuracy: 0.7949\n",
      "\n",
      "Epoch 00050: loss improved from 0.00960 to 0.00746, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 51/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.0081 - accuracy: 0.9962 - val_loss: 1.5245 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00746\n",
      "Epoch 52/100\n",
      "116/116 [==============================] - 57s 492ms/step - loss: 0.0073 - accuracy: 0.9969 - val_loss: 1.6636 - val_accuracy: 0.7684\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00746\n",
      "Epoch 53/100\n",
      "116/116 [==============================] - 61s 528ms/step - loss: 0.0107 - accuracy: 0.9952 - val_loss: 1.5286 - val_accuracy: 0.7949\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00746\n",
      "Epoch 54/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0079 - accuracy: 0.9962 - val_loss: 1.5868 - val_accuracy: 0.7868\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.00746\n",
      "Epoch 55/100\n",
      "116/116 [==============================] - 57s 495ms/step - loss: 0.0100 - accuracy: 0.9948 - val_loss: 1.5602 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00746\n",
      "Epoch 56/100\n",
      "116/116 [==============================] - 58s 504ms/step - loss: 0.0149 - accuracy: 0.9937 - val_loss: 1.1689 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00746\n",
      "Epoch 57/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0108 - accuracy: 0.9943 - val_loss: 1.2778 - val_accuracy: 0.7890\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00746\n",
      "Epoch 58/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 1.3278 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00746\n",
      "Epoch 59/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0072 - accuracy: 0.9968 - val_loss: 1.3705 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.00746\n",
      "Epoch 60/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0065 - accuracy: 0.9972 - val_loss: 1.6013 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00060: loss improved from 0.00746 to 0.00648, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 61/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0062 - accuracy: 0.9968 - val_loss: 1.5717 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.00648\n",
      "Epoch 62/100\n",
      "116/116 [==============================] - 57s 491ms/step - loss: 0.0073 - accuracy: 0.9965 - val_loss: 1.4487 - val_accuracy: 0.7895\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00648\n",
      "Epoch 63/100\n",
      "116/116 [==============================] - 57s 489ms/step - loss: 0.0081 - accuracy: 0.9967 - val_loss: 1.4047 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00648\n",
      "Epoch 64/100\n",
      "116/116 [==============================] - 56s 485ms/step - loss: 0.0129 - accuracy: 0.9943 - val_loss: 1.3140 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00648\n",
      "Epoch 65/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 1.4701 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00648\n",
      "Epoch 66/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0077 - accuracy: 0.9955 - val_loss: 1.4272 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.00648\n",
      "Epoch 67/100\n",
      "116/116 [==============================] - 57s 487ms/step - loss: 0.0116 - accuracy: 0.9945 - val_loss: 1.5038 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.00648\n",
      "Epoch 68/100\n",
      "116/116 [==============================] - 58s 501ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.6116 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00068: loss improved from 0.00648 to 0.00603, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 69/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0061 - accuracy: 0.9973 - val_loss: 1.6893 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.00603\n",
      "Epoch 70/100\n",
      "116/116 [==============================] - 61s 528ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 1.6574 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00603\n",
      "Epoch 71/100\n",
      "116/116 [==============================] - 62s 538ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 1.6847 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00071: loss improved from 0.00603 to 0.00563, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 72/100\n",
      "116/116 [==============================] - 59s 508ms/step - loss: 0.0062 - accuracy: 0.9964 - val_loss: 1.6350 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00563\n",
      "Epoch 73/100\n",
      "116/116 [==============================] - 57s 493ms/step - loss: 0.0054 - accuracy: 0.9970 - val_loss: 1.7188 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.00563\n",
      "Epoch 74/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0093 - accuracy: 0.9961 - val_loss: 1.2403 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.00563\n",
      "Epoch 75/100\n",
      "116/116 [==============================] - 57s 494ms/step - loss: 0.0139 - accuracy: 0.9944 - val_loss: 1.3227 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00563\n",
      "Epoch 76/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0073 - accuracy: 0.9973 - val_loss: 1.5918 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.00563\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0151 - accuracy: 0.9927 - val_loss: 1.3715 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00563\n",
      "Epoch 78/100\n",
      "116/116 [==============================] - 59s 504ms/step - loss: 0.0108 - accuracy: 0.9951 - val_loss: 1.4778 - val_accuracy: 0.7884\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.00563\n",
      "Epoch 79/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0068 - accuracy: 0.9969 - val_loss: 1.6672 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00563\n",
      "Epoch 80/100\n",
      "116/116 [==============================] - 59s 505ms/step - loss: 0.0045 - accuracy: 0.9968 - val_loss: 1.6127 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00563\n",
      "Epoch 81/100\n",
      "116/116 [==============================] - 63s 540ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 1.6913 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00563\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cca0540310>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics = ['accuracy']\n",
    "             )\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                              verbose = 1,\n",
    "                                              patience = 10,\n",
    "                                              mode = 'auto',\n",
    "                                              restore_best_weights = True\n",
    "                                             )\n",
    "\n",
    "checkpoint = ModelCheckpoint('LSTM_models/best_LSTM_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(x=train_text_vec,\n",
    "          y=y_train_label,\n",
    "          class_weight=cws,\n",
    "          batch_size=DEFAULT_BATCH_SIZE,\n",
    "          epochs=DEFAULT_EPOCHS,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1,\n",
    "          validation_data=(\n",
    "              test_text_vec,\n",
    "              y_test_label,\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1246  199]\n",
      " [ 176  227]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1445\n",
      "           1       0.53      0.56      0.55       403\n",
      "\n",
      "    accuracy                           0.80      1848\n",
      "   macro avg       0.70      0.71      0.71      1848\n",
      "weighted avg       0.80      0.80      0.80      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1246  199]\n",
      " [ 176  227]]\n",
      "Accuracy: 0.797077922077922\n",
      "Precision: 0.5328638497652582\n",
      "Recall: 0.5632754342431762\n",
      "F2 Score: 0.5569185475956819\n",
      "AUC Score: 0.8074493204083562\n",
      "\n",
      "{'threshold': 0.01, 'score': 0.6023255813953488, 'y_pred': array([0, 0, 0, ..., 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('LSTM_models/best_LSTM_model.h5')\n",
    "\n",
    "y_test_hat = model.predict(test_text_vec)\n",
    "confusion = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat,axis=1))\n",
    "class_report = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report)\n",
    "\n",
    "perf_metrics = evaluate.performance(y_test, np.argmax(y_test_hat,axis=1), y_test_hat)\n",
    "print(perf_metrics['report'])\n",
    "\n",
    "threshold_metrics = evaluate.threshold(y_test_hat, y_test)\n",
    "print(threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cca04edf10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 53s 416ms/step - loss: 0.6247 - accuracy: 0.5894 - val_loss: 0.3894 - val_accuracy: 0.8247\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.54548, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 60s 520ms/step - loss: 0.3498 - accuracy: 0.8418 - val_loss: 0.3928 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00002: loss improved from 0.54548 to 0.35497, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 61s 530ms/step - loss: 0.2451 - accuracy: 0.8983 - val_loss: 0.4788 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: loss improved from 0.35497 to 0.24929, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 62s 532ms/step - loss: 0.1785 - accuracy: 0.9142 - val_loss: 0.5843 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00004: loss improved from 0.24929 to 0.17787, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.1106 - accuracy: 0.9590 - val_loss: 0.6309 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00005: loss improved from 0.17787 to 0.11824, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 58s 496ms/step - loss: 0.0849 - accuracy: 0.9710 - val_loss: 0.7776 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00006: loss improved from 0.11824 to 0.08743, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0661 - accuracy: 0.9746 - val_loss: 0.8279 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00007: loss improved from 0.08743 to 0.06990, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 60s 516ms/step - loss: 0.0507 - accuracy: 0.9799 - val_loss: 0.8664 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00008: loss improved from 0.06990 to 0.05597, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0434 - accuracy: 0.9837 - val_loss: 0.8620 - val_accuracy: 0.8128\n",
      "\n",
      "Epoch 00009: loss improved from 0.05597 to 0.04819, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 70s 602ms/step - loss: 0.0337 - accuracy: 0.9852 - val_loss: 0.9219 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00010: loss improved from 0.04819 to 0.04009, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 65s 564ms/step - loss: 0.0339 - accuracy: 0.9880 - val_loss: 0.9948 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00011: loss improved from 0.04009 to 0.03941, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 66s 572ms/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 1.0304 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00012: loss improved from 0.03941 to 0.03201, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 68s 587ms/step - loss: 0.0353 - accuracy: 0.9855 - val_loss: 1.1270 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00013: loss improved from 0.03201 to 0.03196, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 65s 560ms/step - loss: 0.0229 - accuracy: 0.9919 - val_loss: 1.1303 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00014: loss improved from 0.03196 to 0.02776, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 65s 557ms/step - loss: 0.0259 - accuracy: 0.9899 - val_loss: 1.2647 - val_accuracy: 0.8166\n",
      "\n",
      "Epoch 00015: loss improved from 0.02776 to 0.02377, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 66s 570ms/step - loss: 0.0180 - accuracy: 0.9927 - val_loss: 1.3384 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00016: loss improved from 0.02377 to 0.02018, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 66s 571ms/step - loss: 0.0180 - accuracy: 0.9928 - val_loss: 1.2932 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.02018\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 61s 522ms/step - loss: 0.0181 - accuracy: 0.9925 - val_loss: 1.4419 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.02018\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0210 - accuracy: 0.9898 - val_loss: 1.2513 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.02018\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 74s 640ms/step - loss: 0.0248 - accuracy: 0.9869 - val_loss: 1.2943 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02018\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0231 - accuracy: 0.9886 - val_loss: 1.2678 - val_accuracy: 0.8155\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.02018\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0143 - accuracy: 0.9929 - val_loss: 1.4124 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00022: loss improved from 0.02018 to 0.01507, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 63s 548ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 1.4509 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00023: loss improved from 0.01507 to 0.01433, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0117 - accuracy: 0.9948 - val_loss: 1.4921 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00024: loss improved from 0.01433 to 0.01116, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 62s 533ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 1.8341 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00025: loss improved from 0.01116 to 0.00950, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 1.6625 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00950\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 60s 522ms/step - loss: 0.0090 - accuracy: 0.9961 - val_loss: 1.5105 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00950\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 63s 540ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 1.7834 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00950\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 72s 624ms/step - loss: 0.0124 - accuracy: 0.9950 - val_loss: 1.3936 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.00950\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 87s 747ms/step - loss: 0.0148 - accuracy: 0.9937 - val_loss: 1.6775 - val_accuracy: 0.7771\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.00950\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 70s 608ms/step - loss: 0.0184 - accuracy: 0.9925 - val_loss: 1.3764 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00950\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 72s 619ms/step - loss: 0.0225 - accuracy: 0.9915 - val_loss: 1.3197 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00950\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 71s 609ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 1.5773 - val_accuracy: 0.7771\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00950\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 71s 613ms/step - loss: 0.0142 - accuracy: 0.9927 - val_loss: 1.5994 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00950\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 70s 605ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 1.7024 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00035: loss improved from 0.00950 to 0.00941, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - 70s 601ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 1.9107 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00036: loss improved from 0.00941 to 0.00858, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0062 - accuracy: 0.9970 - val_loss: 2.1080 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00858\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 74s 634ms/step - loss: 0.0086 - accuracy: 0.9966 - val_loss: 1.8578 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00858\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 70s 603ms/step - loss: 0.0081 - accuracy: 0.9955 - val_loss: 2.4763 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00039: loss improved from 0.00858 to 0.00747, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 69s 593ms/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 1.4621 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.00747\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 69s 594ms/step - loss: 0.0136 - accuracy: 0.9931 - val_loss: 1.4540 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00747\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 70s 608ms/step - loss: 0.0110 - accuracy: 0.9952 - val_loss: 1.6043 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00747\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 68s 586ms/step - loss: 0.0062 - accuracy: 0.9969 - val_loss: 1.6706 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00747\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 67s 582ms/step - loss: 0.0076 - accuracy: 0.9958 - val_loss: 1.7808 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00747\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 1.5432 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00747\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 67s 578ms/step - loss: 0.0121 - accuracy: 0.9937 - val_loss: 1.6317 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00747\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 73s 627ms/step - loss: 0.0083 - accuracy: 0.9966 - val_loss: 1.6877 - val_accuracy: 0.7825\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00747\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0099 - accuracy: 0.9959 - val_loss: 1.6743 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00747\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 71s 608ms/step - loss: 0.0105 - accuracy: 0.9945 - val_loss: 1.7257 - val_accuracy: 0.7814\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00747\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00049: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd012fba60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model_cnn.add(SpatialDropout1D(0.2))\n",
    "model_cnn.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model_cnn.add(Conv1D(64, 4))\n",
    "model_cnn.add(GlobalMaxPool1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_cnn.compile(optimizer = 'adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop_cnn = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                                  verbose = 1,\n",
    "                                                  patience = 10,\n",
    "                                                  mode = 'auto',\n",
    "                                                  restore_best_weights = True\n",
    "                                                 )\n",
    "\n",
    "checkpoint_cnn = ModelCheckpoint('LSTM_models/best_LSTM_CNN_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list_cnn = [checkpoint_cnn, early_stop_cnn]\n",
    "\n",
    "model_cnn.fit(x=train_text_vec,\n",
    "              y=y_train_label,\n",
    "              class_weight=cws,\n",
    "              batch_size=DEFAULT_BATCH_SIZE,\n",
    "              epochs=DEFAULT_EPOCHS,\n",
    "              callbacks=callbacks_list_cnn,\n",
    "              verbose=1,\n",
    "              validation_data=(\n",
    "                  test_text_vec,\n",
    "                  y_test_label\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1045  400]\n",
      " [  95  308]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.72      0.81      1445\n",
      "           1       0.44      0.76      0.55       403\n",
      "\n",
      "    accuracy                           0.73      1848\n",
      "   macro avg       0.68      0.74      0.68      1848\n",
      "weighted avg       0.81      0.73      0.75      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1045  400]\n",
      " [  95  308]]\n",
      "Accuracy: 0.7321428571428571\n",
      "Precision: 0.4350282485875706\n",
      "Recall: 0.7642679900744417\n",
      "F2 Score: 0.6637931034482758\n",
      "AUC Score: 0.8174452849304954\n",
      "\n",
      "{'threshold': 0.03, 'score': 0.6939868204283361, 'y_pred': array([1, 0, 0, ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model_cnn.load_weights('LSTM_models/best_LSTM_CNN_model.h5')\n",
    "\n",
    "y_test_hat_cnn = model_cnn.predict(test_text_vec)\n",
    "confusion_cnn = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat_cnn,axis=1))\n",
    "class_report_cnn = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat_cnn, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion_cnn)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report_cnn)\n",
    "\n",
    "perf_metrics_cnn = evaluate.performance(y_test, np.argmax(y_test_hat_cnn,axis=1), y_test_hat_cnn)\n",
    "print(perf_metrics_cnn['report'])\n",
    "\n",
    "threshold_metrics_cnn = evaluate.threshold(y_test_hat_cnn, y_test)\n",
    "print(threshold_metrics_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cca0298f40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hc1Zn48e87o1GzuiWry3Ivcke4UE01GILJLgktQBKIl5Bkk2ez2fT8kmw2z2Z3k01Y+ME6CT8gtAQw3cTgLMXgKnfJcsWyLFlWs61m9Tm/P2bEKkKyRtLM3Jk77+d55plyj+99bV2/OvPec88RYwxKKaXCn8PqAJRSSvmHJnSllLIJTehKKWUTmtCVUsomNKErpZRNRFl14PT0dFNYWGjV4ZVSKizt2LGjwRiTMdg2yxJ6YWEhJSUlVh1eKaXCkogcH2qbllyUUsomNKErpZRNaEJXSimb0ISulFI2oQldKaVsQhO6UkrZhCZ0pZSyCU3oSillE5rQlVLKJiy7U1Sd3zNbKwO6/zuWFAR0/8q+9NwMXdpDV0opm9CErpRSNqEJXSmlbEITulJK2YQmdKWUsglN6EopZROa0JVSyiY0oSullE1oQldKKZsYNqGLSKyIbBORPSJSJiI/GaTNchFpEpHd3sePAhOuUkqpofhy638ncKUxplVEXMAHIvKmMWbLgHYbjTE3+j9EpZRSvhg2oRtjDNDqfevyPkwgg1JKKTVyPtXQRcQpIruBOuBtY8zWQZot85Zl3hSRoiH2s1pESkSkpL6+fgxhK6WUGsinhG6M6TXGLADygMUiMmdAk53ARGPMfOC/gJeH2M8aY0yxMaY4IyNjLHErpZQaYESjXIwxZ4F3gesGfN5sjGn1vl4HuEQk3V9BKqWUGp4vo1wyRCTF+zoOuBo4MKBNloiI9/Vi734b/R+uUkqpofgyyiUbeEJEnHgS9Z+MMa+LyP0AxphHgVuAL4tID9AO3Oa9mKqUUipIfBnlshdYOMjnj/Z7/RDwkH9DU0opNRJ6p6hSStmEJnSllLIJTehKKWUTmtCVUsomNKErpZRNaEJXSimb0ISulFI2oQldKaVsQhO6UkrZhCZ0pZSyCU3oSillE5rQlVLKJjShK6WUTWhCV0opm9CErpRSNqEJXSmlbEITulJK2YQmdKWUsglN6EopZRPDJnQRiRWRbSKyR0TKROQng7QREXlQRI6IyF4RWRSYcJVSSg1l2EWigU7gSmNMq4i4gA9E5E1jzJZ+ba4HpnkfS4BHvM9KKaWCZNgeuvFo9b51eR9mQLNVwJPetluAFBHJ9m+oSimlzsenGrqIOEVkN1AHvG2M2TqgSS5wot/7Ku9nA/ezWkRKRKSkvr5+tDErpZQahE8J3RjTa4xZAOQBi0VkzoAmMtgfG2Q/a4wxxcaY4oyMjJFHq5RSakgjGuVijDkLvAtcN2BTFZDf730ecHJMkSmllBoRX0a5ZIhIivd1HHA1cGBAs1eBu72jXZYCTcaYGr9Hq5RSaki+jHLJBp4QESeeXwB/Msa8LiL3AxhjHgXWASuBI8A54AsBilcppdQQhk3oxpi9wMJBPn+032sDfMW/oSmllBoJvVNUKaVsQhO6UkrZhCZ0pZSyCU3oSillE5rQlVLKJjShK6WUTWhCV0opm9CErpRSNqEJXSmlbEITulJK2YQmdKWUsglN6EopZROa0JVSyiY0oSullE1oQldKKZvQhK6UUjahCV0ppWxCE7pSStmEL4tE54vIOyJSLiJlIvL1QdosF5EmEdntffwoMOEqpZQaii+LRPcA3zTG7BSRRGCHiLxtjNk/oN1GY8yN/g9RKaWUL4btoRtjaowxO72vW4ByIDfQgSmllBqZEdXQRaQQWAhsHWTzMhHZIyJvikiRH2JTSik1Ar6UXAAQkQTgReAbxpjmAZt3AhONMa0ishJ4GZg2yD5WA6sBCgoKRh20UkqpT/Kphy4iLjzJ/GljzNqB240xzcaYVu/rdYBLRNIHabfGGFNsjCnOyMgYY+hKKaX682WUiwC/B8qNMb8aok2Wtx0isti730Z/BqqUUur8fCm5XAzcBewTkd3ez74HFAAYYx4FbgG+LCI9QDtwmzHGBCBepZRSQxg2oRtjPgBkmDYPAQ/5KyillFIjp3eKKqWUTWhCV0opm9CErpRSNqEJXSmlbEITulJK2YQmdKWUsglN6EopZROa0JVSyiY0oSullE1oQldKKZvQhK6UUjahCV0ppWxCE7pSStmEJnSllLIJTehKKWUTmtCVUsomNKErpZRNaEJXSimb0ISulFI2MWxCF5F8EXlHRMpFpExEvj5IGxGRB0XkiIjsFZFFgQlXKaXUUIZdJBroAb5pjNkpIonADhF52xizv1+b64Fp3scS4BHvs1JKqSAZtodujKkxxuz0vm4ByoHcAc1WAU8ajy1Aiohk+z1apZRSQxpRDV1ECoGFwNYBm3KBE/3eV/HJpI+IrBaREhEpqa+vH1mkSimlzsvnhC4iCcCLwDeMMc0DNw/yR8wnPjBmjTGm2BhTnJGRMbJIlVJKnZdPCV1EXHiS+dPGmLWDNKkC8vu9zwNOjj08pZRSvvJllIsAvwfKjTG/GqLZq8Dd3tEuS4EmY0yNH+NUSik1DF9GuVwM3AXsE5Hd3s++BxQAGGMeBdYBK4EjwDngC/4PVSml1PkMm9CNMR8weI28fxsDfMVfQSmllBo5vVNUKaVswpeSiwqyspNNvLmvBgSmTkigcPw4XE793avsqa2zh0O1LRypa6WloweD4cZ5OSTHuawOLexoQg8hR+tb+ftnd1F2shmnw1Pl2ni4gTiXkzuXFjA5PcHiCJXyr+ONbTy5+Tjt3b3ERzuJj3by/ZdK+elr+/npqiJuvbDA6hDDiib0EHG0vpXb12zBbQz/vKqIzh43UQ4HxxpaWVd6isc/rOCzxfnMyU22OlSl/KK8pplnt1WSHOfi8xcVkpsahwBz85L5tz8f5Nsv7kMQPnth/rD7Uh76PT4EHGto4/Y1W+h1G5790lLuWlZIfHQU0VEOZmQl8XeXTSYnJY5nt1Wyr7rJ6nCVGrPDdS08teU4Wcmx/N3lU8hPi8chgogwLy+F391TzKXT0vn22r2s3VlldbhhQxO6xXp63Xz1mZ30uA3PfGkp0zITP9EmPjqKL148ibzUOF7eVU1ze7cFkSrlHx3dvazdWU16Qgz3XjKJhJhPFgpiXU5+e3cxSyeN57tr91HZeM6CSMOPJnSLPb6pgrKTzfzs5jnMyPpkMu8THeXgMxfk0+N28/LuajwjRZUKP2/sq6G5vZtbLsgjJso5ZLtYl5Nf3TqfKIfwg1dK9Zz3gSZ0C1WfbedXbx/iypkTuH5O1rDt0xNjuHZ2FgdOtbCr8mwQIlTKvw6eambH8TNcNj2D/LT4YdtnJ8fxjytm8P6hel7fqzefD0cTuoX+zyulGAM/uakIzwwLw1s2ZTyF4+N5Y18NHd29AY5QKf/pdRte31vDhMQYrpo5wec/d/eyQubmJvOT1/bT3KHlxvPRhG6RHcfPsKG8jq9dNdWnnkofhwgr52bT3t3Llo8aAxihUv61t+osjW1dXDs7k6gR3FfhdAg/u3kODa2d/GHz8QBGGP40oVvkkXePkhLv4p5lhSP+s3mp8czMSmTj4QY6tZeuwkCv2/A/B+rITo5lVnbSiP/8/PwULpuewf/78Jh+Mz0PTegWOHiqhQ3ltdyzrJBxg1zh98WVMyfQ3t3LZu2lqzDQ1zu/auYEn8uLA3358ik0tHbx/A4dxjgUTegW+O/3jhIf7eTzFxWOeh/aS1fhwm3G1jvvs3RyGgsLUljz/lF6et1+jNA+NKEH2YnT53hlz0luX1xA6rjoMe3rihmeXvqOyjN+ik4p/zt4qoXGti6Wzxh97xxARPjy5VM4cbpdR7wMQRN6kD211XNR575LJ415X/lp8eSnxrH12Gkdo6tC1tZjjSTFRjF7DL3zPlfPymRyxjie3Fwx5n3ZkSb0IOrudfPijiqumjmB7OQ4v+xzyaTx1Ld0cqyhzS/7U8qfGls7OVzbyoWFaR9PODcWDodwx+ICdlae5VBtix8itBdN6EH0l/I6Glq7uG2x/yYbmpuXTJzLyZZjp/22T6X8ZVvFaUTgwsI0v+3z0wtzcTmFP24/4bd92oUm9CD64/ZKspJiuWxaht/26XI6uGBiKvtPNulNFyqkdPe6Kak4w6zsJJL8OLf5+ATPHdNrd1bR2aMDAvrThB4kJ8+2896hej5TnDeimyp8sWRSGm4D2yu0l65CR2l1E+3dvSydPN7v+771wnzOnOvm7f21ft93OBs2s4jIYyJSJyKlQ2xfLiJNIrLb+/iR/8MMfy/sqMJt4LPF/p/beXxCDFMzEth5/IxeHFUhY0flGdLGRTM5fZzf933J1HRyU+K07DKAL13Fx4Hrhmmz0RizwPv46djDshdjDC/sqOKSqekjus1/JBYWpHDmXDfHdZpRFQLOnuviWH0bC/NTxjRUcSgOh3DLBXl8cKSBmqZ2v+8/XA2b0I0x7wP6XX4Mdp84S+Xpc6xakBOwY8zOScLlFHad0FkYlfX2nDiLARbkpwTsGKsW5GAMvKFj0j/mr2LuMhHZIyJvikjRUI1EZLWIlIhISX19vZ8OHfpe21NDtNPBCh+myB2tmCgnRTnJ7Ks+S7feRacsZIxh14mzTEyLZ3xCTMCOMzkjgTm5Sby252TAjhFu/JHQdwITjTHzgf8CXh6qoTFmjTGm2BhTnJHhv5EeocwzZehJls/IICk2sKuYLyxIoaPbzYFTOj5XWefk2Q7qWjpZUBC43nmfm+bnsKeqieONeh8G+CGhG2OajTGt3tfrAJeIpI85MpvYeqyRupZObgpguaXPlIwEkmKj2KVTASgL7TpxBqdDmJcb+IR+wzzP/yvtpXuMOaGLSJZ4r3qIyGLvPnUKQK/X9tQQH+3kqpmZAT+WQ4T5eSkcqm3hXGdPwI+n1EBuY9hT1cTMrETioodeXs5fclPiuLAwlVc1oQO+DVt8FtgMzBCRKhG5V0TuF5H7vU1uAUpFZA/wIHCb0bFzAHT1uHmztIZrZmcG5eQGmJefgtvA/prmoBxPqf6ONbTR1tnDvLzA9877fGp+DodqWzmopUaGnYzbGHP7MNsfAh7yW0Q2suloA2fPdXPjvMCXW/rkJMeSNi6afdVNFPvxdmulfFFa3YTLKczIHHrBc39bOTebH79axhv7as670Hok0DtFA2h9WS3jop1cOi14lxREhDk5yRytb9WyiwoqtzGUnmxmRmYi0VHBSy3pCTEUT0zjrbJTQTtmqNKEHiC9bsPb+2tZPmMCsa7glFv6zM1L1rKLCrq+csvcIJZb+lxblMmBUy0RP9pFE3qA7Ko8Q0NrJ9cWBf5i6ED9yy5KBYsV5ZY+K4o893isj/Beuib0AFlfdgqXU7hi5oSgH1vLLirY3MZQZkG5pU9+Wjyzs5NYXxbZk3VpQg8AYwzry2q5aEp6wG8mGoqWXVQwVTS00drZw5zcZMtiWFGUxc7KM9S1dFgWg9U0oQfAgVMtVJ4+9/HXQCto2UUF0z5vuWVm1tiXmRutFXMyMQY27K+zLAaraUIPgPVlpxCBa2YHv37eR8suKlisLrf0mZGZSEFafETX0TWhB8BbZbVcUJBKRmLgJibyxdxcLbuowAuFcgt4OjErijLZdLQhYlfv0oTuZydOn2N/TbOl5ZY+OSladlGBFwrllj4rirLo7jW8cyAyyy6a0P2s7+ueFcMVB/qrskuXll2U/4VKuaXPooJU0hNieCtCR7tY/xOwmbfKapmZlcjE8f5fdms0Pi67nNSyi/K/UCm39HE4hGtmZ/LuwTo6uiNvAWlN6H7U0NrJ9uOnuTYEyi19tOyiAimUyi19VhRl0tbVy6ajDVaHEnSa0P1ow/5ajPGcUKFCyy4qUPrKLdNDpNzS56Ip6STGRLG+NPLKLqHzU7CB9WWnyEuNY3Z26PRWQMsuKjAqGj3llrkhUm7pEx3l4IqZE9hQXkuvO7Jm8taE7ictHd18eKSRFUVZAVnlfCxyUmJJjXdRelLLLsp/SqubiHJISE5Ze21RJo1tXZRURNb69prQ/eS9Q/V09bpDYrjiQCLCnNxkjtRp2UX5h9sYyqqbmZGVSExUcGcT9cXyGROIjnLw1v7IKrtoQveT9WW1jB8XzQUTU60OZVB9ZZdyvclI+cHxxnO0hNDoloESYqK4ZGo668tOEUkLqGlC94POnl7eOVDH1bMycTpCq9zSJzcljtR4l452UX6xz1tumRmC5ZY+187OpOpMO+U1kbM0nS9rij4mInUiUjrEdhGRB0XkiIjsFZFF/g8ztG0+2khrZw8r5oTO6JaBPh7tUtdGe1fkjc9V/uMZ3dLE9MzQLLf0uXp2JiKRNUe6Lz30x4HrzrP9emCa97EaeGTsYYWXvqXmLpoSvKXmRmNObjK9xmjZRY3J8cZztHSE3uiWgTxL06VGVB192IRujHkfON+l4lXAk8ZjC5AiItn+CjDUuS1cam6k8lLjSNGyixqj0jAot/RZUZRFeU0zJ06fszqUoPBHDT0XONHvfZX3s4iw64R1S82NVF/Z5UhdK03tkTkbnRobt7tfuSXEOzAA186OrKXp/JHQB7sKOOhlZRFZLSIlIlJSX1/vh0Nb762yWsuWmhuNud6yy4YI+hqq/GdH5Rmaw6Dc0qdgfDwzsxIjpuzij4ReBeT3e58HnBysoTFmjTGm2BhTnJGR4YdDW8uz1Nwpllm41NxI5aXGkRLnYt2+GqtDUWHojb01YVNu6XNtURYlFadpbO20OpSA80dCfxW42zvaZSnQZIyJiGxxqLaVisZzITV3y3D6bjLaeDhyFwFQo+N2G94srQmbckufFUWZuA38pdz+c6T7MmzxWWAzMENEqkTkXhG5X0Tu9zZZB3wEHAF+CzwQsGhDzFt9S83NCp+EDp7RLl29bi27qBHZWXmG2ubOkL2ZaCizs5PITYmLiDp61HANjDG3D7PdAF/xW0RhZP3+UyzMT2FCUqzVoYxIfmocOcmxvL63hr9ZlGd1OCpMvLbnJNFRjrAqt4DnW+m1RZk8vbWSts4exsUMm/bClt4pOkrVZ9sprW4OqbnPfSUifGpBDu8fqud0W5fV4agw0N3r5vW9NVwzKzPkh+cOZkVRFl09bt47ZI/BGEPRhD5Kb3m/voXiZFy+uHlBLj1uwxt7B71+rdRf+eBIA41tXaxakGN1KKNSPDGV1HjXx/9v7UoT+iitLzvFtAkJTEoPjaXmRmpWdhIzMhN5ebcmdDW8V3ZVkxznYvmM8BieO1CU08HVszL5y4E6unrcVocTMJrQR6GupYNtx05z/dzwviF21cIcdhw/EzF30anRaevsYX1ZLSvnZofUykQjtXJuNi0dPWw8bN+yS/j+dCz059JTuA3cOC+8E/pN8z1fn1/ZXW1xJCqUvb2/lvbuXm4O03JLn4unppMc5+KNvfYdVa0JfRRe31vD9MwEpmeG19X+gfJS41lcmMZLu6ojas5oNTIv764mJzmWCwvTrA5lTKKjHKwoyuTt/bV0dNtzxlFN6CNU29zB9orT3DA3vHsrfW5emMvR+jb2VOmEXeqTaps7eP9QPTcvzMURonP9j8QN83Jo6ezhfZuOdtGEPkLr9tVgDNwQ5uWWPjfOzybW5eCP208M31hFnBd2VOE28Nni/OEbh4GLpownJd7FGzad+kIT+gi9vreGmVmJTJ2QYHUofpEU62Ll3Gxe23NSF75Qf8UYw/MlJ1g8KY3CMB3NNZDL6eC6oiw22LTsogl9BE6ebWfH8TN8ar49yi19bi3Op7WzRyfsUn9l67HTVDSe41ab9M773Dgvh7Yuz7KRdqMJfQRe9o4G+dQ8eyX0xZPSKBwfzx9LtOyi/tefSk6QGBPFyjAfnjvQ0slppCfE8NIu+43u0oTuI2MMa3dWc2FhKgXj460Ox69EhM8U57Pt2GmONbRZHY4KAc0d3azbV8OnFuQQFx1+t/qfT5TTwc0LcnjnYB1nbDb1hSZ0H5VWN3OkrpVPL7TnZFa3XJCH0yE8t63S6lBUCHhpZzUd3W7blVv6fHpRLt29htdtNvWFJnQfvbiziugoBzfY7Otnn8ykWFYUZfLc9hN6cTTCud2GJzZXMD8/hfn5KVaHExCzs5OYmZXIWpuVXTSh+6C7181re05yzaxMkuPDY2Wi0bhnWSFN7d1652iE23ikgY/q2/j8RROtDiVgRIS/WZTLrsqzfFTfanU4fqMJ3QfvH6qnsa2LTy+099rXiyelMTMrkcc3VeidoxHsiU0VpCdE2+5i6ECrFuTiEHjZRr10Teg++FPJCcaPi+byGeG/Dur5iAifv6iQA6da2HrstNXhKAscb2zjnYN13LFkIjFR9roYOlBmUiwXT03nhR1V9Lrt0YHRhD6M2uYONpTXcUtxHi6n/f+5Vi3IJTnOxeMfVlgdirLAE5uO4xThziUFVocSFHcsLuBkUwfvHbLHmHT7Z6gx+uP2E/S6DXcsjowTPC7ayZ1LCli//xRHbVRbVMM73dbFs9squWl+DplhtqziaF09O5OMxBie3mKP0V0+JXQRuU5EDorIERH5ziDbl4tIk4js9j5+5P9Qg6+n182z2yq5dFo6E8fb49ZnX3zxkknERDl45N2jVoeiguixD47R0dPLA1dMsTqUoHE5HdxanM87B+uoPttudThjNmxCFxEn8DBwPTAbuF1EZg/SdKMxZoH38VM/x2mJdw/WU9PUETFfP/ukJ8Rw++ICXtpVrYtfRIim9m6e2FTB9XOymDohvKeFHqnbFudjgD/a4B4MX3roi4EjxpiPjDFdwHPAqsCGFRqe2VbJhMQYrpqVaXUoQbf6ssk4BP77fe2lR4I/bK6gpbOHB5ZPtTqUoMtLjWf59Aye236C7t7wXp7Ol4SeC/Sf5KPK+9lAy0Rkj4i8KSJFg+1IRFaLSImIlNTXh/Z8xB/Vt/LOwTpuuzA/Ii6GDpSdHMctF+Tzp+1VnGrqsDocFUAtHd089mEFV8zIYE5ustXhWOJzSydS19IZ9hPU+ZKpBpvVfuAYn53ARGPMfOC/gJcH25ExZo0xptgYU5yREdpDAH+78Rgup4O7lhVaHYplHljuqaX+6u2DFkeiAunR945yuq2Lb1w93epQLHPFjAlMyRjHf7/3UVjfg+FLQq8C+k/okAf81QQIxphmY0yr9/U6wCUi6X6LMsjqWzp5cWcVf7soj4zEGKvDsUx+Wjz3XDSR53dUsf9ks9XhqAA4ebad3208xqoFOba9zd8XDoew+rLJ7K9p5sMjjVaHM2q+JPTtwDQRmSQi0cBtwKv9G4hIloiI9/Vi737D9l/liU0VdPe6+dKlk6wOxXJfvWIayXEufr6uPKx7Lmpw/7H+IAb41ooZVodiuZsX5pKRGBPW142GTejGmB7gq8B6oBz4kzGmTETuF5H7vc1uAUpFZA/wIHCbCdP//W2dPfxhy3GunZ3J5Ax7rEo0FsnxLv7+yml8cKSBd226DmOk2lfVxNpd1Xzx4knkpdprSujRiIly8oWLC9l4uIGyk+G5xq5PV/uMMeuMMdONMVOMMf/i/exRY8yj3tcPGWOKjDHzjTFLjTGbAhl0ID215ThN7d2svixyxuIO53NLJzIpfRw/frVMZ2K0iZ5eN999aS/pCdERNe58OHcumUhCTBQPv3PE6lBGJfKGb5xHU3s3//fdoyyfkcEFE1OtDidkREc5+Pmn53K88Ry/fEsvkNrBmo0fUVrdzE9XzSEp1r4ziI5UcpyLey+ZxLp9p9hbddbqcEZME3o/a94/SlN7t9YTB7FsynjuXFLAYx8eY1flGavDUWNwtL6VX284zHVFWbafUXE07rt0Emnjovn39eHXedGE7lXX3MFjH1Rw0/wcinIicyzucL5z/UyykmL5pxf2auklTHX1uPnH5/cQ53Ly05sHvV0k4iXGuvjKFVPZeLiBD480WB3OiGhC9/rNXw7T3evmH66J3LG4w0mMdfGLW+ZxpL6V77+0T0e9hKGfrytnV+VZfnbzHCYkRsYEXKNx55ICcpJj+cWfD+AOo6l1NaEDu0+c5ZltlXxu6UQK0yNnEq7RuHRaBt+4ajprd1Xz1Nbwn/sikryyu5rHN1XwxYsn8an5OVaHE9JiXU6+dd0M9lY18UwYzfES8Qm9u9fNd9fuY0JiDN+8VnvnvvjalVO5YkYGP32tjG26EEZY2FfVxHde3MeFhal8d+VMq8MJCzcvyOXiqeP5xZ8PUNccHtNfRHxCf+yDY5TXNPOTm+aQqFf7feJwCL++dSH5qfHc+/h2SqvDc8xupDhU28Ldj20lbVw0D9+xKCLnJhoNEeFnN8+ls8fNT17bb3U4Ponon+zR+lb+c8MhrpmdyXVzsqwOJ6wkx7v4w31LSIpzcdfvt3K4tsXqkNQgjje28bnfbcXldPDMl5YwIUIWrvCXSenj+Psrp/LGvhr+XHrK6nCGFbEJvb2rl688vZM4l5N/XjXH6nDCUm5KHE/ft4Qop4Pb1mxhx3EdzhhKdp84y98+spnuXjdP3bckohZp8afVl01hXl4y33phD5WNob0+QMQm9B+9UsrB2hZ+fdtCspK11zJahenj+OPqpSTERnH7b7fw2p6Tw/8hFXB/Lq3htjWbiYt28Pz9y5ieGVmLVvhTdJSDh+9YhAAPPLODju7QHbIbkQn9ma2VPL+jiq9dMZXLp4f2NL7hYHJGAi89cDEL8lL42rO7+OHLpZzr6rE6rIh0rquHH75cyv1P7WRWdhIvPXBxxK1AFAj5afH88rMLKK1u5sevloXskN2IS+jr9tXwg5f3cfn0DL4ewfM/+1vauGj+cN9i7r1kEk9tPc7K32xk09Hwuikj3H1wuIEbHvyAp7Ye575LJvHsl5aSnhC50z/72zWzM/nqFVN5bvsJ/iNEp8CIsjqAYHrvUD1ff24XiwpSeeRzi3A6Blu7Q41WTJSTH944m6tnZfKtF/Zwx2+3snxGBv+0Yiazc5KsDs+2Squb+Lf1B3n/UD35aXE8c99Slk0Zb3VYtvTNa7mveM4AAAnbSURBVKfT2NbJw+8cJTHWxf2Xh9bEZhGT0F/dc5J/fH4P0yYk8vvPX0h8dMT81YNu2ZTxbPiHy3lycwUPv3OUlQ9u5KIp47nnokKunDlBh835QUd3L38pr+OJTRVsqzhNcpyLH9wwi7uWTSQmyml1eLbVN5SxpaOHf33zAC0d3Xzzmhk4QqRzaPusZozhN385zK83HGZxYRqP3nUByXE63jzQYl1OVl82hVuLC3h623Ge2nycv/vDDpLjXFw7O5NrZmeyZNJ4kuP1Z+GrxtZONn/UyIb9tWwor6O1s4f8tDh+cMMsPlOcr+d1kDgdwn/euoDE2Cgefucoxxra+OVnFhAXbf0vUlsn9MrGc3xn7V42HW3kbxfl8fO/maO9lyBLjnfxwPKprL50Mu8erGeddzzv8zuqEIHZ2UnMzk5iRlYiM7M8z5G87B94OiH1rZ2U17RwoKaZA6da2H+ymYPesf7JcS5umJvNDfOyuXhqupYOLeByeqaUnpyewM/fLOdAzUZ+ccs8LixMszQusepqbXFxsSkpKQnIvls7e3hiUwUP/c8RnA7heytncfvifLyr5IWFZ8J8npQ7lhQMua2zp5fdlWfZ/FEjJRVnOHCqmYbWro+3J8e5yE6OJSs5lqwkz3N6QgxJcS6SYqO8zy6S4qJIinUR6wr9X9Jut6Gls4fm9m6avI+z5zzPja2dnGzqoKapnZqzHZxsaqel439HCWUnxzIzK5HiwjQumjKeubnJRJ2nbGXncycUfXikgW+/uJfqs+3csbiAr105LaBDoUVkhzGmeNBtdkroJ06f44UdVTy+qYKm9m6unjWBf755DtnJcX49TjCE+3/KkWrt7KG2uYNTTR00tHZ6El9HN83tPbR2nn8IpMspxEY5iXE5iPE+xw54jolyEOV04BTBIZ7pC5wiOB2Cw+H5zCny8ecOh2CMwW2g120wxtBrDL1uTw+61+1573YbOnvcdHT30tHtprNn4LObzu5e2rp6ON+kfeOinSTHu0iOiyY5zsX4cdGeX2pJscTH2PqL9CeEW0IHz9KV/77+IH/YchynCLcU53HH4gKKcpL83pE8X0IP2zOl121oaO1k/8lmdlWe4f3DDew+4Vlh5OpZmXz1yqksiOBVzMNNQkwUCRkJTBlkHdcet5tzXb10dPXS0d1Le7fb+9z7cSLtcbvp6TV097rpcRt6et20dPRwpreLbu/nbuNJxgZwG4MxQzwDDgHBk/BFPHVTh3geTgf9Xgux3l8ksS4H8dFRpI37318sfZ+Pi44iJd5FUpyLvSeaiIt2EudyEhftJD7aqReKw9y4mCh+fFMR914yiUfeO8oLJVU8s7WSwvHxXDUrk4UFKczLTSE7JTagP2ufeugich3wG8AJ/M4Y868Dtot3+0rgHPB5Y8zO8+1ztD30Dftr+eErpdS1dNLr7fI4BIpyklk5N5sb52WTnxb+C95GWg9dqT7h2EMf6Oy5Lv5ceorX99awveI0nT1uAERg/LgY7r1kEl9eProhj2PqoYuIE3gYuAaoAraLyKvGmP7Tj10PTPM+lgCPeJ/9LjMploumpJOVHENmUizTMxOZm5vMuAj7WqqUCl0p8dHctriA2xYX0NXj5sCpZsprmqlp6qC2uYOCAHU6fcmCi4EjxpiPAETkOWAV0D+hrwKeNJ7u/hYRSRGRbGNMjb8DnpuXzC8/O9/fu1VKqYCIjnIwLy+FeXmBLwH7ktBzgRP93lfxyd73YG1ygb9K6CKyGljtfdsqIqFw/2w6EIr3qGtcIxOqcUHoxhaScd0ZonEROnFNHGqDLwl9sEu0AwvvvrTBGLMGWOPDMYNGREqGqkdZSeMamVCNC0I3No1rZEI1rv58udxaBeT3e58HDJwj1Zc2SimlAsiXhL4dmCYik0QkGrgNeHVAm1eBu8VjKdAUiPq5UkqpoQ1bcjHG9IjIV4H1eIYtPmaMKROR+73bHwXW4RmyeATPsMUvBC5kvwupElA/GtfIhGpcELqxaVwjE6pxfcyyO0WVUkr5l96eppRSNqEJXSmlbCLiErqIpInI2yJy2PucOkibfBF5R0TKRaRMRL4ewHiuE5GDInJERL4zyHYRkQe92/eKyKJAxTLCuO70xrNXRDaJSFDu9hourn7tLhSRXhG5JVTiEpHlIrLbe069F4y4fIlNRJJF5DUR2eONLeDXwETkMRGpE5HSIbZbdd4PF5cl573PjDER9QD+DfiO9/V3gF8M0iYbWOR9nQgcAmYHIBYncBSYDEQDewYeB8/F5jfxjPVfCmwNwr+RL3FdBKR6X18fKnH1a/c/eC7W3xIKcQEpeO6uLvC+nxDouEYQ2/f6/h8AGcBpIDrAcV0GLAJKh9ge9PPex7iCft6P5BFxPXQ80xQ84X39BHDzwAbGmBrjnVzMGNMClOO589XfPp5WwRjTBfRNqzAw3ieNxxYgRUSyAxDLiOIyxmwyxpzxvt2C596DQPPl3wvga8CLQF0QYvI1rjuAtcaYSgBjTCjFZoBE7yR7CXgS+vnnLB4jY8z73uMMxYrzfti4LDrvfRaJCT3TeMfIe58nnK+xiBQCC4GtAYhlqCkTRtrGirj6uxdPbyrQho1LRHKBTwOPBiEen+MCpgOpIvKuiOwQkbtDKLaHgFl4bgbcB3zdGOMOTnhDsuK8H6lgnfc+s+UUhSKyAcgaZNP3R7ifBDw9vW8YY5r9EdvAQwzy2aimVfAzn48pIlfgObEvCWhE3sMN8tnAuH4NfNsY0xvEFap8iSsKuAC4CogDNovIFmPMoRCIbQWwG7gSmAK8LSIbA3TO+8qK895nQT7vfWbLhG6MuXqobSJS2zcTpPcr3KBffUXEhSeZP22MWRugUEN1WgWfjiki84DfAdcbYxoDHJOvcRUDz3mTeTqwUkR6jDEvWxxXFdBgjGkD2kTkfWA+nuszgeRLbF8A/tV4CsNHROQYMBPYFuDYzidkpxOx4Lz3WSSWXF4F7vG+vgd4ZWADby3x90C5MeZXAYwlVKdVGDYuESkA1gJ3BaGX6XNcxphJxphCY0wh8ALwQICTuU9x4TnPLhWRKBGJxzNjaXmA4/I1tko83xwQkUxgBvBREGI7n5CcTsSi8953Vl+VDfYDGA/8BTjsfU7zfp4DrPO+vgTP17u9eL6K7gZWBiielXh6aUeB73s/ux+43/ta8CwwchRPfbM4SP9Ow8X1O+BMv3+fklCIa0DbxwnCKBdf4wK+hWekSymeMl6wzvnhfpY5wFve86sU+FwQYnoWz/Ta3Xh64/eGyHk/XFyWnPe+PvTWf6WUsolILLkopZQtaUJXSimb0ISulFI2oQldKaVsQhO6UkrZhCZ0pZSyCU3oSillE/8fRaeL1uYIDrcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(y_test_hat_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

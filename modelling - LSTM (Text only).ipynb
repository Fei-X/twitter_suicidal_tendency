{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7221\n",
       "1    2017\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things to take note before using this modelling notebook\n",
    "\n",
    "1. There are 2 approaches to using the text corpus\n",
    "    a. LDA: Hard to explain because clusters are not labelled but dimensionality has been reduced to 5 (based on grid)\n",
    "    b. TFIDF: 173 (based on vectorizer) tfidf float numbers exist per tweet. Easier explanability but high dimensionality)\n",
    "2. A new feature \"day_after\" has been added. Remember to include it in the modelling step if you wish to.\n",
    "3. Remember to do scaling on numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girls i like e...\n",
       "2    content warnings for billies documentary  stro...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pays principle is a threat to thi...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girl i like em...\n",
       "2    content warning for billy documentary  strong ...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pay principle is a threat to this...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet              object\n",
       "label               int64\n",
       "day                 int64\n",
       "nlikes              int64\n",
       "nreplies            int64\n",
       "nretweets           int64\n",
       "reply_to            int64\n",
       "url                 int64\n",
       "join_time           int64\n",
       "tweets              int64\n",
       "following           int64\n",
       "followers           int64\n",
       "likes               int64\n",
       "media               int64\n",
       "day_after           int64\n",
       "tweet_length        int64\n",
       "tweet_sentiment     int64\n",
       "bio_sentiment       int64\n",
       "first_person        int64\n",
       "second_person       int64\n",
       "third_person        int64\n",
       "text_vec           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = df['label']\n",
    "data_x = df['tweet']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, stratify=data_y, random_state = 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run keras Tokenizer\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train_tok = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training constants\n",
    "MAX_SEQ_LEN = 53 # Based on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vec = pad_sequences(x_train_tok, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(x_test_tok, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 14216\n",
      "Max Token Index: 14216 \n",
      "\n",
      "Sample Tweet Before Processing: we can at least prevent people from such suicidal act  using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda\n",
      "Sample Tweet After Processing: ['we can at least prevent people from such suicidal act using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 45, 43, 358, 788, 27, 42, 296, 1, 372, 470, 2691, 2146, 79, 768, 1153, 4575, 1957, 4576, 2691, 4577, 1958, 3629, 4578, 1957, 4579, 4580]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', x_train.values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_label = encoder.fit_transform(y_train.values)\n",
    "y_train_label = to_categorical(y_train_label) \n",
    "\n",
    "y_test_label = encoder.fit_transform(y_test.values)\n",
    "y_test_label = to_categorical(y_test_label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get class weights for the training data, this will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({0: 5776, 1: 1614})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ctr = Counter(y_train.values)\n",
    "print('Distribution of Classes:', ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6397160664819944, 1: 2.2893432465923174}\n"
     ]
    }
   ],
   "source": [
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train_label,axis=1)\n",
    "cws_raw = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "label = [0,1]\n",
    "\n",
    "cws = dict(zip(label, cws_raw))\n",
    "\n",
    "print(cws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "58/58 [==============================] - 32s 485ms/step - loss: 0.6598 - accuracy: 0.5105 - val_loss: 0.4609 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.59100, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 35s 605ms/step - loss: 0.4038 - accuracy: 0.8182 - val_loss: 0.3849 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00002: loss improved from 0.59100 to 0.39309, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 41s 700ms/step - loss: 0.2991 - accuracy: 0.8669 - val_loss: 0.4298 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00003: loss improved from 0.39309 to 0.29655, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 38s 649ms/step - loss: 0.2294 - accuracy: 0.8955 - val_loss: 0.5609 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00004: loss improved from 0.29655 to 0.23611, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 37s 643ms/step - loss: 0.1705 - accuracy: 0.9227 - val_loss: 0.4807 - val_accuracy: 0.8063\n",
      "\n",
      "Epoch 00005: loss improved from 0.23611 to 0.18397, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 38s 645ms/step - loss: 0.1428 - accuracy: 0.9454 - val_loss: 0.6245 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00006: loss improved from 0.18397 to 0.13678, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 37s 632ms/step - loss: 0.0857 - accuracy: 0.9642 - val_loss: 0.7527 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00007: loss improved from 0.13678 to 0.08906, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 36s 615ms/step - loss: 0.0677 - accuracy: 0.9699 - val_loss: 0.7742 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00008: loss improved from 0.08906 to 0.07393, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 36s 629ms/step - loss: 0.0705 - accuracy: 0.9725 - val_loss: 0.8514 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00009: loss improved from 0.07393 to 0.06550, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 36s 618ms/step - loss: 0.0514 - accuracy: 0.9804 - val_loss: 0.8115 - val_accuracy: 0.7863\n",
      "\n",
      "Epoch 00010: loss improved from 0.06550 to 0.06299, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 35s 611ms/step - loss: 0.0565 - accuracy: 0.9776 - val_loss: 0.9712 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00011: loss improved from 0.06299 to 0.05737, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 36s 616ms/step - loss: 0.0549 - accuracy: 0.9779 - val_loss: 1.1463 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00012: loss improved from 0.05737 to 0.04828, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 35s 603ms/step - loss: 0.0421 - accuracy: 0.9827 - val_loss: 1.1792 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00013: loss improved from 0.04828 to 0.04301, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 36s 618ms/step - loss: 0.0305 - accuracy: 0.9877 - val_loss: 0.8877 - val_accuracy: 0.7798\n",
      "\n",
      "Epoch 00014: loss improved from 0.04301 to 0.03752, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 39s 667ms/step - loss: 0.0486 - accuracy: 0.9785 - val_loss: 1.2072 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.03752\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 39s 674ms/step - loss: 0.0341 - accuracy: 0.9873 - val_loss: 1.1095 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00016: loss improved from 0.03752 to 0.03476, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 43s 741ms/step - loss: 0.0244 - accuracy: 0.9893 - val_loss: 1.3180 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00017: loss improved from 0.03476 to 0.02774, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 44s 765ms/step - loss: 0.0309 - accuracy: 0.9874 - val_loss: 1.0516 - val_accuracy: 0.7890\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.02774\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 48s 825ms/step - loss: 0.0265 - accuracy: 0.9893 - val_loss: 1.2652 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.02774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dbda82ffa0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 128\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.002, patience=2, \n",
    "                                              verbose=0, mode='auto', baseline=None)\n",
    "checkpoint = ModelCheckpoint('LSTM_models/best_LSTM_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(x=train_text_vec,\n",
    "          y=y_train_label,\n",
    "          class_weight=cws,\n",
    "          batch_size=DEFAULT_BATCH_SIZE,\n",
    "          epochs=DEFAULT_EPOCHS,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1,\n",
    "          validation_data=(\n",
    "              test_text_vec,\n",
    "              y_test_label\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 2s 34ms/step - loss: 1.3180 - accuracy: 0.7992\n",
      "Test loss:\n",
      " 1.3180272579193115\n",
      "\n",
      "\n",
      "Test accuracy:\n",
      " 0.7992424368858337\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[1270  175]\n",
      " [ 196  207]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      1445\n",
      "           1       0.54      0.51      0.53       403\n",
      "\n",
      "    accuracy                           0.80      1848\n",
      "   macro avg       0.70      0.70      0.70      1848\n",
      "weighted avg       0.80      0.80      0.80      1848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('LSTM_models/best_LSTM_model.h5')\n",
    "results = model.evaluate(test_text_vec, y_test_label)\n",
    "print(\"Test loss:\\n\", results[0])\n",
    "print(\"\\n\")\n",
    "print(\"Test accuracy:\\n\", results[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test_hat = model.predict(test_text_vec)\n",
    "confusion = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat,axis=1))\n",
    "class_report = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "58/58 [==============================] - 56s 879ms/step - loss: 0.6460 - accuracy: 0.5657 - val_loss: 0.4252 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.56902, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 50s 865ms/step - loss: 0.3633 - accuracy: 0.8357 - val_loss: 0.3818 - val_accuracy: 0.8306\n",
      "\n",
      "Epoch 00002: loss improved from 0.56902 to 0.35814, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 49s 839ms/step - loss: 0.2543 - accuracy: 0.8907 - val_loss: 0.4295 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00003: loss improved from 0.35814 to 0.25431, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 53s 918ms/step - loss: 0.1735 - accuracy: 0.9217 - val_loss: 0.5297 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00004: loss improved from 0.25431 to 0.17871, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 56s 970ms/step - loss: 0.1156 - accuracy: 0.9523 - val_loss: 0.6156 - val_accuracy: 0.8128\n",
      "\n",
      "Epoch 00005: loss improved from 0.17871 to 0.12314, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 49s 837ms/step - loss: 0.0956 - accuracy: 0.9642 - val_loss: 0.7028 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00006: loss improved from 0.12314 to 0.09564, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 48s 826ms/step - loss: 0.0634 - accuracy: 0.9754 - val_loss: 0.8592 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00007: loss improved from 0.09564 to 0.06468, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 51s 873ms/step - loss: 0.0495 - accuracy: 0.9792 - val_loss: 0.8420 - val_accuracy: 0.7835\n",
      "\n",
      "Epoch 00008: loss improved from 0.06468 to 0.05896, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 49s 846ms/step - loss: 0.0656 - accuracy: 0.9731 - val_loss: 0.9373 - val_accuracy: 0.8187\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.05896\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 48s 816ms/step - loss: 0.0467 - accuracy: 0.9821 - val_loss: 0.8406 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00010: loss improved from 0.05896 to 0.04942, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 48s 836ms/step - loss: 0.0366 - accuracy: 0.9857 - val_loss: 1.0633 - val_accuracy: 0.7863\n",
      "\n",
      "Epoch 00011: loss improved from 0.04942 to 0.03797, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 48s 829ms/step - loss: 0.0361 - accuracy: 0.9863 - val_loss: 1.1522 - val_accuracy: 0.8160\n",
      "\n",
      "Epoch 00012: loss improved from 0.03797 to 0.03777, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 47s 817ms/step - loss: 0.0365 - accuracy: 0.9839 - val_loss: 1.1934 - val_accuracy: 0.8090\n",
      "\n",
      "Epoch 00013: loss improved from 0.03777 to 0.03285, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 48s 823ms/step - loss: 0.0224 - accuracy: 0.9915 - val_loss: 0.9624 - val_accuracy: 0.7949\n",
      "\n",
      "Epoch 00014: loss improved from 0.03285 to 0.03069, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 48s 826ms/step - loss: 0.0380 - accuracy: 0.9859 - val_loss: 0.9548 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.03069\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 48s 831ms/step - loss: 0.0299 - accuracy: 0.9871 - val_loss: 1.1220 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.03069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dc01ab26d0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 128\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model_cnn.add(SpatialDropout1D(0.2))\n",
    "model_cnn.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model_cnn.add(Conv1D(64, 4))\n",
    "model_cnn.add(GlobalMaxPool1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop_cnn = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.002, patience=2, \n",
    "                                              verbose=0, mode='auto', baseline=None)\n",
    "checkpoint_cnn = ModelCheckpoint('LSTM_models/best_LSTM_CNN_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "callbacks_list_cnn = [checkpoint_cnn, early_stop_cnn]\n",
    "\n",
    "model_cnn.fit(x=train_text_vec,\n",
    "              y=y_train_label,\n",
    "              class_weight=cws,\n",
    "              batch_size=DEFAULT_BATCH_SIZE,\n",
    "              epochs=DEFAULT_EPOCHS,\n",
    "              callbacks=callbacks_list_cnn,\n",
    "              verbose=1,\n",
    "              validation_data=(\n",
    "                  test_text_vec,\n",
    "                  y_test_label\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 2s 36ms/step - loss: 0.9624 - accuracy: 0.7949\n",
      "Test loss:\n",
      " 0.9623579382896423\n",
      "\n",
      "\n",
      "Test accuracy:\n",
      " 0.7949134111404419\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[1229  216]\n",
      " [ 163  240]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.87      1445\n",
      "           1       0.53      0.60      0.56       403\n",
      "\n",
      "    accuracy                           0.79      1848\n",
      "   macro avg       0.70      0.72      0.71      1848\n",
      "weighted avg       0.81      0.79      0.80      1848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cnn.load_weights('LSTM_models/best_LSTM_CNN_model.h5')\n",
    "results_cnn = model_cnn.evaluate(test_text_vec, y_test_label)\n",
    "print(\"Test loss:\\n\", results_cnn[0])\n",
    "print(\"\\n\")\n",
    "print(\"Test accuracy:\\n\", results_cnn[1])\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test_hat_cnn = model_cnn.predict(test_text_vec)\n",
    "confusion_cnn = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat_cnn,axis=1))\n",
    "class_report_cnn = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat_cnn, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion_cnn)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

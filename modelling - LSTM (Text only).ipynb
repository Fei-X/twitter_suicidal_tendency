{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7221\n",
       "1    2017\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things to take note before using this modelling notebook\n",
    "\n",
    "1. There are 2 approaches to using the text corpus\n",
    "    a. LDA: Hard to explain because clusters are not labelled but dimensionality has been reduced to 5 (based on grid)\n",
    "    b. TFIDF: 173 (based on vectorizer) tfidf float numbers exist per tweet. Easier explanability but high dimensionality)\n",
    "2. A new feature \"day_after\" has been added. Remember to include it in the modelling step if you wish to.\n",
    "3. Remember to do scaling on numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girls i like e...\n",
       "2    content warnings for billies documentary  stro...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pays principle is a threat to thi...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\OVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girl i like em...\n",
       "2    content warning for billy documentary  strong ...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pay principle is a threat to this...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet              object\n",
       "label               int64\n",
       "day                 int64\n",
       "nlikes              int64\n",
       "nreplies            int64\n",
       "nretweets           int64\n",
       "reply_to            int64\n",
       "url                 int64\n",
       "join_time           int64\n",
       "tweets              int64\n",
       "following           int64\n",
       "followers           int64\n",
       "likes               int64\n",
       "media               int64\n",
       "day_after           int64\n",
       "tweet_length        int64\n",
       "tweet_sentiment     int64\n",
       "bio_sentiment       int64\n",
       "first_person        int64\n",
       "second_person       int64\n",
       "third_person        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = df['label']\n",
    "data_x = df['tweet']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, stratify=data_y, random_state = 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run keras Tokenizer\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train_tok = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('LSTM_models/tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training constants\n",
    "MAX_SEQ_LEN = 53 # Based on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vec = pad_sequences(x_train_tok, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(x_test_tok, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 14216\n",
      "Max Token Index: 14216 \n",
      "\n",
      "Sample Tweet Before Processing: we can at least prevent people from such suicidal act  using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda\n",
      "Sample Tweet After Processing: ['we can at least prevent people from such suicidal act using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 45, 43, 358, 788, 27, 42, 296, 1, 372, 470, 2691, 2146, 79, 768, 1153, 4575, 1957, 4576, 2691, 4577, 1958, 3629, 4578, 1957, 4579, 4580]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', x_train.values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_label = encoder.fit_transform(y_train.values)\n",
    "y_train_label = to_categorical(y_train_label) \n",
    "\n",
    "y_test_label = encoder.fit_transform(y_test.values)\n",
    "y_test_label = to_categorical(y_test_label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get class weights for the training data, this will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({0: 5776, 1: 1614})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ctr = Counter(y_train.values)\n",
    "print('Distribution of Classes:', ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6397160664819944, 1: 2.2893432465923174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OVO\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train_label,axis=1)\n",
    "cws_raw = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "label = [0,1]\n",
    "\n",
    "cws = dict(zip(label, cws_raw))\n",
    "\n",
    "print(cws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 53s 414ms/step - loss: 0.6398 - accuracy: 0.5507 - val_loss: 0.4279 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.56624, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 53s 461ms/step - loss: 0.3855 - accuracy: 0.8225 - val_loss: 0.4150 - val_accuracy: 0.8209\n",
      "\n",
      "Epoch 00002: loss improved from 0.56624 to 0.38841, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 53s 457ms/step - loss: 0.2898 - accuracy: 0.8736 - val_loss: 0.4010 - val_accuracy: 0.8144\n",
      "\n",
      "Epoch 00003: loss improved from 0.38841 to 0.29063, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 53s 457ms/step - loss: 0.2130 - accuracy: 0.9054 - val_loss: 0.4754 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00004: loss improved from 0.29063 to 0.20948, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 53s 453ms/step - loss: 0.1578 - accuracy: 0.9337 - val_loss: 0.5349 - val_accuracy: 0.8122\n",
      "\n",
      "Epoch 00005: loss improved from 0.20948 to 0.16417, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 55s 477ms/step - loss: 0.1180 - accuracy: 0.9530 - val_loss: 0.7447 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00006: loss improved from 0.16417 to 0.11353, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 54s 468ms/step - loss: 0.0835 - accuracy: 0.9667 - val_loss: 0.6751 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00007: loss improved from 0.11353 to 0.08735, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 53s 460ms/step - loss: 0.0854 - accuracy: 0.9649 - val_loss: 0.7985 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00008: loss improved from 0.08735 to 0.08411, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0579 - accuracy: 0.9766 - val_loss: 0.8978 - val_accuracy: 0.8074\n",
      "\n",
      "Epoch 00009: loss improved from 0.08411 to 0.05828, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 56s 487ms/step - loss: 0.0606 - accuracy: 0.9773 - val_loss: 0.8615 - val_accuracy: 0.7652\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.05828\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 63s 543ms/step - loss: 0.0641 - accuracy: 0.9728 - val_loss: 1.0328 - val_accuracy: 0.7825\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.05828\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 61s 527ms/step - loss: 0.0997 - accuracy: 0.9550 - val_loss: 1.0121 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.05828\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 61s 525ms/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 0.9319 - val_accuracy: 0.8144\n",
      "\n",
      "Epoch 00013: loss improved from 0.05828 to 0.05281, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 62s 531ms/step - loss: 0.0474 - accuracy: 0.9808 - val_loss: 1.0715 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00014: loss improved from 0.05281 to 0.04674, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 61s 526ms/step - loss: 0.0313 - accuracy: 0.9877 - val_loss: 1.1283 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00015: loss improved from 0.04674 to 0.03202, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 61s 523ms/step - loss: 0.0317 - accuracy: 0.9876 - val_loss: 1.2691 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.03202\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 62s 534ms/step - loss: 0.0260 - accuracy: 0.9898 - val_loss: 1.0839 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.03202\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 62s 531ms/step - loss: 0.0282 - accuracy: 0.9876 - val_loss: 1.1780 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00018: loss improved from 0.03202 to 0.02732, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 60s 513ms/step - loss: 0.0218 - accuracy: 0.9912 - val_loss: 1.2649 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00019: loss improved from 0.02732 to 0.02725, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 1.1455 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02725\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 62s 532ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 1.0295 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00021: loss improved from 0.02725 to 0.02633, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 60s 516ms/step - loss: 0.0327 - accuracy: 0.9862 - val_loss: 1.1965 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.02633\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0187 - accuracy: 0.9926 - val_loss: 1.2177 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00023: loss improved from 0.02633 to 0.02122, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 60s 520ms/step - loss: 0.0216 - accuracy: 0.9895 - val_loss: 1.2608 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.02122\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 59s 510ms/step - loss: 0.0197 - accuracy: 0.9920 - val_loss: 1.3434 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00025: loss improved from 0.02122 to 0.02114, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 60s 515ms/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 1.3724 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00026: loss improved from 0.02114 to 0.01651, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.0195 - accuracy: 0.9937 - val_loss: 1.3489 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01651\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 59s 510ms/step - loss: 0.0148 - accuracy: 0.9934 - val_loss: 1.4957 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01651\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0155 - accuracy: 0.9923 - val_loss: 1.3453 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00029: loss improved from 0.01651 to 0.01641, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 61s 530ms/step - loss: 0.0263 - accuracy: 0.9894 - val_loss: 1.2504 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01641\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0244 - accuracy: 0.9919 - val_loss: 0.9660 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01641\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 66s 571ms/step - loss: 0.0299 - accuracy: 0.9883 - val_loss: 1.1433 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01641\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 65s 560ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 1.2264 - val_accuracy: 0.7982\n",
      "\n",
      "Epoch 00033: loss improved from 0.01641 to 0.01535, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 60s 515ms/step - loss: 0.0143 - accuracy: 0.9936 - val_loss: 1.2998 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00034: loss improved from 0.01535 to 0.01272, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 57s 494ms/step - loss: 0.0108 - accuracy: 0.9951 - val_loss: 1.4616 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00035: loss improved from 0.01272 to 0.01192, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0361 - accuracy: 0.9863 - val_loss: 1.1838 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01192\n",
      "Epoch 37/100\n",
      "116/116 [==============================] - 59s 505ms/step - loss: 0.0175 - accuracy: 0.9913 - val_loss: 1.2356 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01192\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 58s 501ms/step - loss: 0.0122 - accuracy: 0.9948 - val_loss: 1.2768 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.01192\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0408 - accuracy: 0.9842 - val_loss: 0.9510 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.01192\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 62s 534ms/step - loss: 0.0234 - accuracy: 0.9894 - val_loss: 1.3178 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01192\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 58s 499ms/step - loss: 0.0117 - accuracy: 0.9959 - val_loss: 1.2172 - val_accuracy: 0.7781\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.01192\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0179 - accuracy: 0.9923 - val_loss: 1.2588 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.01192\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0088 - accuracy: 0.9968 - val_loss: 1.3279 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00043: loss improved from 0.01192 to 0.01080, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 59s 509ms/step - loss: 0.0090 - accuracy: 0.9964 - val_loss: 1.4419 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00044: loss improved from 0.01080 to 0.00960, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 59s 508ms/step - loss: 0.0091 - accuracy: 0.9958 - val_loss: 1.4182 - val_accuracy: 0.7890\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00960\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 59s 513ms/step - loss: 0.0114 - accuracy: 0.9954 - val_loss: 1.4443 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00960\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 59s 513ms/step - loss: 0.0149 - accuracy: 0.9936 - val_loss: 1.3381 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00960\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 59s 512ms/step - loss: 0.0106 - accuracy: 0.9958 - val_loss: 1.2640 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00960\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 58s 499ms/step - loss: 0.0134 - accuracy: 0.9934 - val_loss: 1.3064 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00960\n",
      "Epoch 50/100\n",
      "116/116 [==============================] - 63s 547ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 1.4729 - val_accuracy: 0.7949\n",
      "\n",
      "Epoch 00050: loss improved from 0.00960 to 0.00746, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 51/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.0081 - accuracy: 0.9962 - val_loss: 1.5245 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00746\n",
      "Epoch 52/100\n",
      "116/116 [==============================] - 57s 492ms/step - loss: 0.0073 - accuracy: 0.9969 - val_loss: 1.6636 - val_accuracy: 0.7684\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00746\n",
      "Epoch 53/100\n",
      "116/116 [==============================] - 61s 528ms/step - loss: 0.0107 - accuracy: 0.9952 - val_loss: 1.5286 - val_accuracy: 0.7949\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00746\n",
      "Epoch 54/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0079 - accuracy: 0.9962 - val_loss: 1.5868 - val_accuracy: 0.7868\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.00746\n",
      "Epoch 55/100\n",
      "116/116 [==============================] - 57s 495ms/step - loss: 0.0100 - accuracy: 0.9948 - val_loss: 1.5602 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00746\n",
      "Epoch 56/100\n",
      "116/116 [==============================] - 58s 504ms/step - loss: 0.0149 - accuracy: 0.9937 - val_loss: 1.1689 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00746\n",
      "Epoch 57/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0108 - accuracy: 0.9943 - val_loss: 1.2778 - val_accuracy: 0.7890\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00746\n",
      "Epoch 58/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 1.3278 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00746\n",
      "Epoch 59/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0072 - accuracy: 0.9968 - val_loss: 1.3705 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.00746\n",
      "Epoch 60/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0065 - accuracy: 0.9972 - val_loss: 1.6013 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00060: loss improved from 0.00746 to 0.00648, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 61/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0062 - accuracy: 0.9968 - val_loss: 1.5717 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.00648\n",
      "Epoch 62/100\n",
      "116/116 [==============================] - 57s 491ms/step - loss: 0.0073 - accuracy: 0.9965 - val_loss: 1.4487 - val_accuracy: 0.7895\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.00648\n",
      "Epoch 63/100\n",
      "116/116 [==============================] - 57s 489ms/step - loss: 0.0081 - accuracy: 0.9967 - val_loss: 1.4047 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.00648\n",
      "Epoch 64/100\n",
      "116/116 [==============================] - 56s 485ms/step - loss: 0.0129 - accuracy: 0.9943 - val_loss: 1.3140 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.00648\n",
      "Epoch 65/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 1.4701 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.00648\n",
      "Epoch 66/100\n",
      "116/116 [==============================] - 58s 498ms/step - loss: 0.0077 - accuracy: 0.9955 - val_loss: 1.4272 - val_accuracy: 0.7754\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.00648\n",
      "Epoch 67/100\n",
      "116/116 [==============================] - 57s 487ms/step - loss: 0.0116 - accuracy: 0.9945 - val_loss: 1.5038 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.00648\n",
      "Epoch 68/100\n",
      "116/116 [==============================] - 58s 501ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.6116 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00068: loss improved from 0.00648 to 0.00603, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 69/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0061 - accuracy: 0.9973 - val_loss: 1.6893 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.00603\n",
      "Epoch 70/100\n",
      "116/116 [==============================] - 61s 528ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 1.6574 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.00603\n",
      "Epoch 71/100\n",
      "116/116 [==============================] - 62s 538ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 1.6847 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00071: loss improved from 0.00603 to 0.00563, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 72/100\n",
      "116/116 [==============================] - 59s 508ms/step - loss: 0.0062 - accuracy: 0.9964 - val_loss: 1.6350 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.00563\n",
      "Epoch 73/100\n",
      "116/116 [==============================] - 57s 493ms/step - loss: 0.0054 - accuracy: 0.9970 - val_loss: 1.7188 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.00563\n",
      "Epoch 74/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0093 - accuracy: 0.9961 - val_loss: 1.2403 - val_accuracy: 0.7592\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.00563\n",
      "Epoch 75/100\n",
      "116/116 [==============================] - 57s 494ms/step - loss: 0.0139 - accuracy: 0.9944 - val_loss: 1.3227 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.00563\n",
      "Epoch 76/100\n",
      "116/116 [==============================] - 58s 500ms/step - loss: 0.0073 - accuracy: 0.9973 - val_loss: 1.5918 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.00563\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0151 - accuracy: 0.9927 - val_loss: 1.3715 - val_accuracy: 0.7830\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.00563\n",
      "Epoch 78/100\n",
      "116/116 [==============================] - 59s 504ms/step - loss: 0.0108 - accuracy: 0.9951 - val_loss: 1.4778 - val_accuracy: 0.7884\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.00563\n",
      "Epoch 79/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0068 - accuracy: 0.9969 - val_loss: 1.6672 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00563\n",
      "Epoch 80/100\n",
      "116/116 [==============================] - 59s 505ms/step - loss: 0.0045 - accuracy: 0.9968 - val_loss: 1.6127 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.00563\n",
      "Epoch 81/100\n",
      "116/116 [==============================] - 63s 540ms/step - loss: 0.0060 - accuracy: 0.9963 - val_loss: 1.6913 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00563\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00081: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cca0540310>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics = ['accuracy']\n",
    "             )\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                              verbose = 1,\n",
    "                                              patience = 10,\n",
    "                                              mode = 'auto',\n",
    "                                              restore_best_weights = True\n",
    "                                             )\n",
    "\n",
    "checkpoint = ModelCheckpoint('LSTM_models/best_LSTM_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(x=train_text_vec,\n",
    "          y=y_train_label,\n",
    "          class_weight=cws,\n",
    "          batch_size=DEFAULT_BATCH_SIZE,\n",
    "          epochs=DEFAULT_EPOCHS,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1,\n",
    "          validation_data=(\n",
    "              test_text_vec,\n",
    "              y_test_label,\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 53, 128)           1819776   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 53, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,099,522\n",
      "Trainable params: 2,099,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model = keras.models.load_model(\"LSTM_models/best_LSTM_model.h5\")\n",
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1246  199]\n",
      " [ 176  227]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87      1445\n",
      "           1       0.53      0.56      0.55       403\n",
      "\n",
      "    accuracy                           0.80      1848\n",
      "   macro avg       0.70      0.71      0.71      1848\n",
      "weighted avg       0.80      0.80      0.80      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1246  199]\n",
      " [ 176  227]]\n",
      "Accuracy: 0.797077922077922\n",
      "Precision: 0.5328638497652582\n",
      "Recall: 0.5632754342431762\n",
      "F2 Score: 0.5569185475956819\n",
      "AUC Score: 0.8074493204083562\n",
      "\n",
      "{'threshold': 0.01, 'score': 0.6023255813953488, 'y_pred': array([0, 0, 0, ..., 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('LSTM_models/best_LSTM_model.h5')\n",
    "\n",
    "y_test_hat = model.predict(test_text_vec)\n",
    "confusion = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat,axis=1))\n",
    "class_report = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report)\n",
    "\n",
    "perf_metrics = evaluate.performance(y_test, np.argmax(y_test_hat,axis=1), y_test_hat)\n",
    "print(perf_metrics['report'])\n",
    "\n",
    "threshold_metrics = evaluate.threshold(y_test_hat, y_test)\n",
    "print(threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.distplot(y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 53s 416ms/step - loss: 0.6247 - accuracy: 0.5894 - val_loss: 0.3894 - val_accuracy: 0.8247\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.54548, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 60s 520ms/step - loss: 0.3498 - accuracy: 0.8418 - val_loss: 0.3928 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00002: loss improved from 0.54548 to 0.35497, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 61s 530ms/step - loss: 0.2451 - accuracy: 0.8983 - val_loss: 0.4788 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00003: loss improved from 0.35497 to 0.24929, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 62s 532ms/step - loss: 0.1785 - accuracy: 0.9142 - val_loss: 0.5843 - val_accuracy: 0.8133\n",
      "\n",
      "Epoch 00004: loss improved from 0.24929 to 0.17787, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 60s 518ms/step - loss: 0.1106 - accuracy: 0.9590 - val_loss: 0.6309 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00005: loss improved from 0.17787 to 0.11824, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 58s 496ms/step - loss: 0.0849 - accuracy: 0.9710 - val_loss: 0.7776 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00006: loss improved from 0.11824 to 0.08743, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0661 - accuracy: 0.9746 - val_loss: 0.8279 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00007: loss improved from 0.08743 to 0.06990, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 60s 516ms/step - loss: 0.0507 - accuracy: 0.9799 - val_loss: 0.8664 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00008: loss improved from 0.06990 to 0.05597, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 59s 507ms/step - loss: 0.0434 - accuracy: 0.9837 - val_loss: 0.8620 - val_accuracy: 0.8128\n",
      "\n",
      "Epoch 00009: loss improved from 0.05597 to 0.04819, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 70s 602ms/step - loss: 0.0337 - accuracy: 0.9852 - val_loss: 0.9219 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00010: loss improved from 0.04819 to 0.04009, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 65s 564ms/step - loss: 0.0339 - accuracy: 0.9880 - val_loss: 0.9948 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00011: loss improved from 0.04009 to 0.03941, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 66s 572ms/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 1.0304 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00012: loss improved from 0.03941 to 0.03201, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 68s 587ms/step - loss: 0.0353 - accuracy: 0.9855 - val_loss: 1.1270 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00013: loss improved from 0.03201 to 0.03196, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 65s 560ms/step - loss: 0.0229 - accuracy: 0.9919 - val_loss: 1.1303 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00014: loss improved from 0.03196 to 0.02776, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 65s 557ms/step - loss: 0.0259 - accuracy: 0.9899 - val_loss: 1.2647 - val_accuracy: 0.8166\n",
      "\n",
      "Epoch 00015: loss improved from 0.02776 to 0.02377, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 66s 570ms/step - loss: 0.0180 - accuracy: 0.9927 - val_loss: 1.3384 - val_accuracy: 0.8149\n",
      "\n",
      "Epoch 00016: loss improved from 0.02377 to 0.02018, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 66s 571ms/step - loss: 0.0180 - accuracy: 0.9928 - val_loss: 1.2932 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.02018\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 61s 522ms/step - loss: 0.0181 - accuracy: 0.9925 - val_loss: 1.4419 - val_accuracy: 0.7803\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.02018\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 63s 546ms/step - loss: 0.0210 - accuracy: 0.9898 - val_loss: 1.2513 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.02018\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 74s 640ms/step - loss: 0.0248 - accuracy: 0.9869 - val_loss: 1.2943 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02018\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0231 - accuracy: 0.9886 - val_loss: 1.2678 - val_accuracy: 0.8155\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.02018\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 59s 506ms/step - loss: 0.0143 - accuracy: 0.9929 - val_loss: 1.4124 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00022: loss improved from 0.02018 to 0.01507, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 63s 548ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 1.4509 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00023: loss improved from 0.01507 to 0.01433, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 58s 503ms/step - loss: 0.0117 - accuracy: 0.9948 - val_loss: 1.4921 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00024: loss improved from 0.01433 to 0.01116, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 62s 533ms/step - loss: 0.0109 - accuracy: 0.9954 - val_loss: 1.8341 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00025: loss improved from 0.01116 to 0.00950, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 58s 502ms/step - loss: 0.0098 - accuracy: 0.9959 - val_loss: 1.6625 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.00950\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 60s 522ms/step - loss: 0.0090 - accuracy: 0.9961 - val_loss: 1.5105 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.00950\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 63s 540ms/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 1.7834 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.00950\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 72s 624ms/step - loss: 0.0124 - accuracy: 0.9950 - val_loss: 1.3936 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.00950\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 87s 747ms/step - loss: 0.0148 - accuracy: 0.9937 - val_loss: 1.6775 - val_accuracy: 0.7771\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.00950\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 70s 608ms/step - loss: 0.0184 - accuracy: 0.9925 - val_loss: 1.3764 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.00950\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 72s 619ms/step - loss: 0.0225 - accuracy: 0.9915 - val_loss: 1.3197 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.00950\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 71s 609ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 1.5773 - val_accuracy: 0.7771\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.00950\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 71s 613ms/step - loss: 0.0142 - accuracy: 0.9927 - val_loss: 1.5994 - val_accuracy: 0.7971\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.00950\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 70s 605ms/step - loss: 0.0087 - accuracy: 0.9954 - val_loss: 1.7024 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00035: loss improved from 0.00950 to 0.00941, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 36/100\n",
      "116/116 [==============================] - 70s 601ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 1.9107 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00036: loss improved from 0.00941 to 0.00858, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0062 - accuracy: 0.9970 - val_loss: 2.1080 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.00858\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 74s 634ms/step - loss: 0.0086 - accuracy: 0.9966 - val_loss: 1.8578 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.00858\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 70s 603ms/step - loss: 0.0081 - accuracy: 0.9955 - val_loss: 2.4763 - val_accuracy: 0.7321\n",
      "\n",
      "Epoch 00039: loss improved from 0.00858 to 0.00747, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 69s 593ms/step - loss: 0.0349 - accuracy: 0.9880 - val_loss: 1.4621 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.00747\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 69s 594ms/step - loss: 0.0136 - accuracy: 0.9931 - val_loss: 1.4540 - val_accuracy: 0.8019\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.00747\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 70s 608ms/step - loss: 0.0110 - accuracy: 0.9952 - val_loss: 1.6043 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.00747\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 68s 586ms/step - loss: 0.0062 - accuracy: 0.9969 - val_loss: 1.6706 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.00747\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 67s 582ms/step - loss: 0.0076 - accuracy: 0.9958 - val_loss: 1.7808 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00747\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 1.5432 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.00747\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 67s 578ms/step - loss: 0.0121 - accuracy: 0.9937 - val_loss: 1.6317 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.00747\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 73s 627ms/step - loss: 0.0083 - accuracy: 0.9966 - val_loss: 1.6877 - val_accuracy: 0.7825\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00747\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 69s 596ms/step - loss: 0.0099 - accuracy: 0.9959 - val_loss: 1.6743 - val_accuracy: 0.7998\n",
      "\n",
      "Epoch 00048: loss did not improve from 0.00747\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 71s 608ms/step - loss: 0.0105 - accuracy: 0.9945 - val_loss: 1.7257 - val_accuracy: 0.7814\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00747\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00049: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd012fba60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model_cnn.add(SpatialDropout1D(0.2))\n",
    "model_cnn.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model_cnn.add(Conv1D(64, 4))\n",
    "model_cnn.add(GlobalMaxPool1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_cnn.compile(optimizer = 'adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop_cnn = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                                  verbose = 1,\n",
    "                                                  patience = 10,\n",
    "                                                  mode = 'auto',\n",
    "                                                  restore_best_weights = True\n",
    "                                                 )\n",
    "\n",
    "checkpoint_cnn = ModelCheckpoint('LSTM_models/best_LSTM_CNN_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list_cnn = [checkpoint_cnn, early_stop_cnn]\n",
    "\n",
    "model_cnn.fit(x=train_text_vec,\n",
    "              y=y_train_label,\n",
    "              class_weight=cws,\n",
    "              batch_size=DEFAULT_BATCH_SIZE,\n",
    "              epochs=DEFAULT_EPOCHS,\n",
    "              callbacks=callbacks_list_cnn,\n",
    "              verbose=1,\n",
    "              validation_data=(\n",
    "                  test_text_vec,\n",
    "                  y_test_label\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 53, 128)           1819776   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 53, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 53, 256)           263168    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,152,834\n",
      "Trainable params: 2,152,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model_cnn = keras.models.load_model(\"LSTM_models/best_LSTM_CNN_model.h5\")\n",
    "reconstructed_model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1045  400]\n",
      " [  95  308]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.72      0.81      1445\n",
      "           1       0.44      0.76      0.55       403\n",
      "\n",
      "    accuracy                           0.73      1848\n",
      "   macro avg       0.68      0.74      0.68      1848\n",
      "weighted avg       0.81      0.73      0.75      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1045  400]\n",
      " [  95  308]]\n",
      "Accuracy: 0.7321428571428571\n",
      "Precision: 0.4350282485875706\n",
      "Recall: 0.7642679900744417\n",
      "F2 Score: 0.6637931034482758\n",
      "AUC Score: 0.8174452849304954\n",
      "\n",
      "{'threshold': 0.03, 'score': 0.6939868204283361, 'y_pred': array([1, 0, 0, ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model_cnn.load_weights('LSTM_models/best_LSTM_CNN_model.h5')\n",
    "\n",
    "y_test_hat_cnn = model_cnn.predict(test_text_vec)\n",
    "confusion_cnn = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat_cnn,axis=1))\n",
    "class_report_cnn = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat_cnn, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion_cnn)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report_cnn)\n",
    "\n",
    "perf_metrics_cnn = evaluate.performance(y_test, np.argmax(y_test_hat_cnn,axis=1), y_test_hat_cnn)\n",
    "print(perf_metrics_cnn['report'])\n",
    "\n",
    "threshold_metrics_cnn = evaluate.threshold(y_test_hat_cnn, y_test)\n",
    "print(threshold_metrics_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.distplot(y_test_hat_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

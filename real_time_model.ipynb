{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing + Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' #please put your GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(2021)\n",
    "tf.random.set_seed(2021)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7221\n",
       "1    2017\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things to take note before using this modelling notebook\n",
    "\n",
    "1. There are 2 approaches to using the text corpus\n",
    "    a. LDA: Hard to explain because clusters are not labelled but dimensionality has been reduced to 5 (based on grid)\n",
    "    b. TFIDF: 173 (based on vectorizer) tfidf float numbers exist per tweet. Easier explanability but high dimensionality)\n",
    "2. A new feature \"day_after\" has been added. Remember to include it in the modelling step if you wish to.\n",
    "3. Remember to do scaling on numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flyxs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girls i like e...\n",
       "2    content warnings for billies documentary  stro...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pays principle is a threat to thi...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n",
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\flyxs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    service connected covid19 pandemic impacting t...\n",
       "1    im not gone lie ion like normal girl i like em...\n",
       "2    content warning for billy documentary  strong ...\n",
       "3    why am i helping my suicidal irl im literally ...\n",
       "4    the polluter pay principle is a threat to this...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet              object\n",
       "label               int64\n",
       "day                 int64\n",
       "nlikes              int64\n",
       "nreplies            int64\n",
       "nretweets           int64\n",
       "reply_to            int64\n",
       "url                 int64\n",
       "join_time           int64\n",
       "tweets              int64\n",
       "following           int64\n",
       "followers           int64\n",
       "likes               int64\n",
       "media               int64\n",
       "day_after           int64\n",
       "tweet_length        int64\n",
       "tweet_sentiment     int64\n",
       "bio_sentiment       int64\n",
       "first_person        int64\n",
       "second_person       int64\n",
       "third_person        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = df['label']\n",
    "data_x = df['tweet']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size = 0.2, stratify=data_y, random_state = 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.apply(lambda x : len(x.split(' '))).quantile(0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run keras Tokenizer\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train_tok = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tok = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('LSTM_models/tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training constants\n",
    "MAX_SEQ_LEN = 53 # Based on above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_vec = pad_sequences(x_train_tok, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(x_test_tok, maxlen=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 14216\n",
      "Max Token Index: 14216 \n",
      "\n",
      "Sample Tweet Before Processing: we can at least prevent people from such suicidal act  using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda\n",
      "Sample Tweet After Processing: ['we can at least prevent people from such suicidal act using coronil remains their choice eventually marne ke baad coronil kaam ka nahi bol ke kya fayda'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 45, 43, 358, 788, 27, 42, 296, 1, 372, 470, 2691, 2146, 79, 768, 1153, 4575, 1957, 4576, 2691, 4577, 1958, 3629, 4578, 1957, 4579, 4580]\n"
     ]
    }
   ],
   "source": [
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', x_train.values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y_train_label = encoder.fit_transform(y_train.values)\n",
    "y_train_label = to_categorical(y_train_label) \n",
    "\n",
    "y_test_label = encoder.fit_transform(y_test.values)\n",
    "y_test_label = to_categorical(y_test_label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get class weights for the training data, this will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({0: 5776, 1: 1614})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ctr = Counter(y_train.values)\n",
    "print('Distribution of Classes:', ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6397160664819944, 1: 2.2893432465923174}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\flyxs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 0 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train_label,axis=1)\n",
    "cws_raw = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "label = [0,1]\n",
    "\n",
    "cws = dict(zip(label, cws_raw))\n",
    "\n",
    "print(cws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1356   89]\n",
      " [ 272  131]]\n",
      "Accuracy: 0.8046536796536796\n",
      "Precision: 0.5954545454545455\n",
      "Recall: 0.3250620347394541\n",
      "F2 Score: 0.3575327510917031\n",
      "AUC Score: 0.755799496853186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(['suicidal'])\n",
    "count_vectorizer = CountVectorizer(min_df=0.01, stop_words=my_stop_words)\n",
    "count = CountVectorizer(min_df=0.01, stop_words=my_stop_words)\n",
    "x_train_2 = count.fit_transform(x_train)\n",
    "x_test_2 = count.transform(x_test)\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "mnb = naive_bayes.MultinomialNB()\n",
    "mnb.fit(x_train_2, y_train)\n",
    "y_pred_proba = mnb.predict_proba(x_test_2)\n",
    "# dic = evaluate.threshold(y_pred_proba,y_test)\n",
    "# print('Selected threshold: ', dic['threshold'])\n",
    "print(evaluate.performance(y_test, mnb.predict(x_test_2),y_pred_proba)['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wa' 'people' 'thought' 'like' 'just' 'im' 'dont' 'suicide' 'amp' 'know']\n",
      "['im' 'thought' 'wa' 'just' 'like' 'dont' 'feel' 'tw' 'want' 'really']\n"
     ]
    }
   ],
   "source": [
    "neg_class_keyword = mnb.feature_log_prob_[0, :].argsort()[::-1]\n",
    "pos_class_keyword = mnb.feature_log_prob_[1, :].argsort()[::-1]\n",
    "print(np.take(count.get_feature_names(),neg_class_keyword[:10]))\n",
    "print(np.take(count.get_feature_names(),pos_class_keyword[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\flyxs\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5045: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Epoch 1/100\n",
      "116/116 [==============================] - 33s 256ms/step - loss: 0.6364 - accuracy: 0.5298 - val_loss: 0.5126 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.56433, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 33s 281ms/step - loss: 0.3934 - accuracy: 0.8104 - val_loss: 0.3946 - val_accuracy: 0.8263\n",
      "\n",
      "Epoch 00002: loss improved from 0.56433 to 0.38446, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 33s 283ms/step - loss: 0.2853 - accuracy: 0.8744 - val_loss: 0.3840 - val_accuracy: 0.8301\n",
      "\n",
      "Epoch 00003: loss improved from 0.38446 to 0.28755, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 33s 283ms/step - loss: 0.2037 - accuracy: 0.9111 - val_loss: 0.4885 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00004: loss improved from 0.28755 to 0.20649, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.1467 - accuracy: 0.9430 - val_loss: 0.5619 - val_accuracy: 0.8106\n",
      "\n",
      "Epoch 00005: loss improved from 0.20649 to 0.14999, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.1090 - accuracy: 0.9561 - val_loss: 0.6242 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00006: loss improved from 0.14999 to 0.10866, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 34s 289ms/step - loss: 0.0756 - accuracy: 0.9691 - val_loss: 0.7579 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00007: loss improved from 0.10866 to 0.07769, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 34s 290ms/step - loss: 0.0672 - accuracy: 0.9726 - val_loss: 0.8772 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00008: loss improved from 0.07769 to 0.06479, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 34s 292ms/step - loss: 0.0556 - accuracy: 0.9806 - val_loss: 0.8264 - val_accuracy: 0.7927\n",
      "\n",
      "Epoch 00009: loss improved from 0.06479 to 0.05763, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 34s 291ms/step - loss: 0.0515 - accuracy: 0.9792 - val_loss: 0.7823 - val_accuracy: 0.7722\n",
      "\n",
      "Epoch 00010: loss did not improve from 0.05763\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 34s 290ms/step - loss: 0.0644 - accuracy: 0.9733 - val_loss: 0.7935 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.05763\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.0461 - accuracy: 0.9828 - val_loss: 0.8397 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00012: loss improved from 0.05763 to 0.05444, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.0418 - accuracy: 0.9847 - val_loss: 1.0912 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00013: loss improved from 0.05444 to 0.04120, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 34s 294ms/step - loss: 0.0333 - accuracy: 0.9871 - val_loss: 1.1123 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00014: loss improved from 0.04120 to 0.03619, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 36s 309ms/step - loss: 0.0498 - accuracy: 0.9812 - val_loss: 1.0398 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.03619\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 38s 326ms/step - loss: 0.0354 - accuracy: 0.9884 - val_loss: 0.9727 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.03619\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 37s 320ms/step - loss: 0.0244 - accuracy: 0.9902 - val_loss: 1.1158 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00017: loss improved from 0.03619 to 0.02551, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 36s 313ms/step - loss: 0.0272 - accuracy: 0.9888 - val_loss: 1.0908 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.02551\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 35s 305ms/step - loss: 0.0240 - accuracy: 0.9905 - val_loss: 1.1402 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.02551\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 36s 307ms/step - loss: 0.0242 - accuracy: 0.9898 - val_loss: 1.1817 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02551\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 35s 302ms/step - loss: 0.0269 - accuracy: 0.9887 - val_loss: 1.1754 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.02551\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 36s 311ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 1.1597 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00022: loss improved from 0.02551 to 0.02243, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 38s 330ms/step - loss: 0.0185 - accuracy: 0.9927 - val_loss: 1.0449 - val_accuracy: 0.8003\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.02243\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 40s 343ms/step - loss: 0.0244 - accuracy: 0.9912 - val_loss: 1.3882 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00024: loss improved from 0.02243 to 0.02175, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 40s 342ms/step - loss: 0.0185 - accuracy: 0.9917 - val_loss: 1.3482 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00025: loss improved from 0.02175 to 0.01792, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 37s 320ms/step - loss: 0.0106 - accuracy: 0.9952 - val_loss: 1.4741 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00026: loss improved from 0.01792 to 0.01383, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 36s 309ms/step - loss: 0.0204 - accuracy: 0.9914 - val_loss: 1.1880 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01383\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 36s 312ms/step - loss: 0.0128 - accuracy: 0.9952 - val_loss: 1.3144 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01383\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 36s 310ms/step - loss: 0.0138 - accuracy: 0.9932 - val_loss: 1.2517 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01383\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 36s 314ms/step - loss: 0.0364 - accuracy: 0.9863 - val_loss: 1.2087 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01383\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 62s 538ms/step - loss: 0.0193 - accuracy: 0.9919 - val_loss: 1.3552 - val_accuracy: 0.7841\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01383\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 78s 673ms/step - loss: 0.0236 - accuracy: 0.9907 - val_loss: 1.2425 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01383\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 45s 388ms/step - loss: 0.0240 - accuracy: 0.9926 - val_loss: 1.1566 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.01383\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 36s 312ms/step - loss: 0.0145 - accuracy: 0.9946 - val_loss: 1.3265 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00034: loss improved from 0.01383 to 0.01347, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 36s 314ms/step - loss: 0.0088 - accuracy: 0.9962 - val_loss: 1.3895 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00035: loss improved from 0.01347 to 0.01138, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 37s 315ms/step - loss: 0.0114 - accuracy: 0.9953 - val_loss: 1.3516 - val_accuracy: 0.7868\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.01138\n",
      "Epoch 37/100\n",
      "116/116 [==============================] - 37s 315ms/step - loss: 0.0104 - accuracy: 0.9953 - val_loss: 1.4645 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.01138\n",
      "Epoch 38/100\n",
      "116/116 [==============================] - 37s 315ms/step - loss: 0.0133 - accuracy: 0.9931 - val_loss: 1.2329 - val_accuracy: 0.7473\n",
      "\n",
      "Epoch 00038: loss did not improve from 0.01138\n",
      "Epoch 39/100\n",
      "116/116 [==============================] - 37s 316ms/step - loss: 0.0490 - accuracy: 0.9821 - val_loss: 1.2716 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.01138\n",
      "Epoch 40/100\n",
      "116/116 [==============================] - 37s 315ms/step - loss: 0.0302 - accuracy: 0.9893 - val_loss: 1.2740 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00040: loss did not improve from 0.01138\n",
      "Epoch 41/100\n",
      "116/116 [==============================] - 37s 322ms/step - loss: 0.0161 - accuracy: 0.9935 - val_loss: 1.1529 - val_accuracy: 0.7900\n",
      "\n",
      "Epoch 00041: loss did not improve from 0.01138\n",
      "Epoch 42/100\n",
      "116/116 [==============================] - 39s 337ms/step - loss: 0.0152 - accuracy: 0.9956 - val_loss: 1.2503 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00042: loss did not improve from 0.01138\n",
      "Epoch 43/100\n",
      "116/116 [==============================] - 40s 341ms/step - loss: 0.0070 - accuracy: 0.9973 - val_loss: 1.3025 - val_accuracy: 0.7911\n",
      "\n",
      "Epoch 00043: loss improved from 0.01138 to 0.00974, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 44/100\n",
      "116/116 [==============================] - 38s 330ms/step - loss: 0.0101 - accuracy: 0.9954 - val_loss: 1.3514 - val_accuracy: 0.7933\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.00974\n",
      "Epoch 45/100\n",
      "116/116 [==============================] - 36s 314ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 1.4761 - val_accuracy: 0.7982\n",
      "\n",
      "Epoch 00045: loss improved from 0.00974 to 0.00864, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 46/100\n",
      "116/116 [==============================] - 37s 318ms/step - loss: 0.0081 - accuracy: 0.9963 - val_loss: 1.5247 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00046: loss improved from 0.00864 to 0.00812, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 47/100\n",
      "116/116 [==============================] - 37s 318ms/step - loss: 0.0084 - accuracy: 0.9963 - val_loss: 1.5013 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00047: loss did not improve from 0.00812\n",
      "Epoch 48/100\n",
      "116/116 [==============================] - 69s 594ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 1.6809 - val_accuracy: 0.7922\n",
      "\n",
      "Epoch 00048: loss improved from 0.00812 to 0.00797, saving model to LSTM_models\\best_LSTM_model.h5\n",
      "Epoch 49/100\n",
      "116/116 [==============================] - 78s 669ms/step - loss: 0.0107 - accuracy: 0.9947 - val_loss: 1.6076 - val_accuracy: 0.8047\n",
      "\n",
      "Epoch 00049: loss did not improve from 0.00797\n",
      "Epoch 50/100\n",
      "116/116 [==============================] - 77s 665ms/step - loss: 0.0084 - accuracy: 0.9956 - val_loss: 1.6636 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00050: loss did not improve from 0.00797\n",
      "Epoch 51/100\n",
      "116/116 [==============================] - 78s 674ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 1.3609 - val_accuracy: 0.7863\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.00797\n",
      "Epoch 52/100\n",
      "116/116 [==============================] - 77s 667ms/step - loss: 0.0161 - accuracy: 0.9946 - val_loss: 1.3082 - val_accuracy: 0.8063\n",
      "\n",
      "Epoch 00052: loss did not improve from 0.00797\n",
      "Epoch 53/100\n",
      "116/116 [==============================] - 78s 672ms/step - loss: 0.0165 - accuracy: 0.9923 - val_loss: 1.2945 - val_accuracy: 0.7857\n",
      "\n",
      "Epoch 00053: loss did not improve from 0.00797\n",
      "Epoch 54/100\n",
      "116/116 [==============================] - 77s 667ms/step - loss: 0.0109 - accuracy: 0.9943 - val_loss: 1.5115 - val_accuracy: 0.7776\n",
      "\n",
      "Epoch 00054: loss did not improve from 0.00797\n",
      "Epoch 55/100\n",
      "116/116 [==============================] - 43s 370ms/step - loss: 0.0098 - accuracy: 0.9953 - val_loss: 1.6102 - val_accuracy: 0.7771\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.00797\n",
      "Epoch 56/100\n",
      "116/116 [==============================] - 37s 318ms/step - loss: 0.0166 - accuracy: 0.9926 - val_loss: 1.4618 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.00797\n",
      "Epoch 57/100\n",
      "116/116 [==============================] - 37s 317ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 1.4918 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.00797\n",
      "Epoch 58/100\n",
      "116/116 [==============================] - 37s 317ms/step - loss: 0.0117 - accuracy: 0.9958 - val_loss: 1.5465 - val_accuracy: 0.7944\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.00797\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00058: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f8d87c288>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics = ['accuracy']\n",
    "             )\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                              verbose = 1,\n",
    "                                              patience = 10,\n",
    "                                              mode = 'auto',\n",
    "                                              restore_best_weights = True\n",
    "                                             )\n",
    "\n",
    "checkpoint = ModelCheckpoint('LSTM_models/best_LSTM_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(x=train_text_vec,\n",
    "          y=y_train_label,\n",
    "          class_weight=cws,\n",
    "          batch_size=DEFAULT_BATCH_SIZE,\n",
    "          epochs=DEFAULT_EPOCHS,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1,\n",
    "          validation_data=(\n",
    "              test_text_vec,\n",
    "              y_test_label,\n",
    "          ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 53, 128)           1819776   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 53, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,099,522\n",
      "Trainable params: 2,099,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model = keras.models.load_model(\"LSTM_models/best_LSTM_model.h5\")\n",
    "reconstructed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1216  229]\n",
      " [ 155  248]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86      1445\n",
      "           1       0.52      0.62      0.56       403\n",
      "\n",
      "    accuracy                           0.79      1848\n",
      "   macro avg       0.70      0.73      0.71      1848\n",
      "weighted avg       0.81      0.79      0.80      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1216  229]\n",
      " [ 155  248]]\n",
      "Accuracy: 0.7922077922077922\n",
      "Precision: 0.519916142557652\n",
      "Recall: 0.6153846153846154\n",
      "F2 Score: 0.5935854475825755\n",
      "AUC Score: 0.8153227952982391\n",
      "\n",
      "{'threshold': 0.01, 'score': 0.6377204884667571, 'y_pred': array([0, 0, 0, ..., 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('LSTM_models/best_LSTM_model.h5')\n",
    "\n",
    "y_test_hat = model.predict(test_text_vec)\n",
    "confusion = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat,axis=1))\n",
    "class_report = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report)\n",
    "\n",
    "perf_metrics = evaluate.performance(y_test, np.argmax(y_test_hat,axis=1), y_test_hat)\n",
    "print(perf_metrics['report'])\n",
    "\n",
    "threshold_metrics = evaluate.threshold(y_test_hat, y_test)\n",
    "print(threshold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.distplot(y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with CNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "116/116 [==============================] - 36s 288ms/step - loss: 0.6197 - accuracy: 0.5962 - val_loss: 0.3650 - val_accuracy: 0.8398\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.53590, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 2/100\n",
      "116/116 [==============================] - 33s 288ms/step - loss: 0.3430 - accuracy: 0.8453 - val_loss: 0.3816 - val_accuracy: 0.8328\n",
      "\n",
      "Epoch 00002: loss improved from 0.53590 to 0.34871, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 3/100\n",
      "116/116 [==============================] - 35s 304ms/step - loss: 0.2461 - accuracy: 0.8889 - val_loss: 0.4249 - val_accuracy: 0.8274\n",
      "\n",
      "Epoch 00003: loss improved from 0.34871 to 0.24780, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 4/100\n",
      "116/116 [==============================] - 36s 311ms/step - loss: 0.1591 - accuracy: 0.9272 - val_loss: 0.5283 - val_accuracy: 0.8182\n",
      "\n",
      "Epoch 00004: loss improved from 0.24780 to 0.16822, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 5/100\n",
      "116/116 [==============================] - 36s 311ms/step - loss: 0.0980 - accuracy: 0.9631 - val_loss: 0.5431 - val_accuracy: 0.8014\n",
      "\n",
      "Epoch 00005: loss improved from 0.16822 to 0.11233, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 6/100\n",
      "116/116 [==============================] - 36s 310ms/step - loss: 0.0814 - accuracy: 0.9654 - val_loss: 0.7900 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00006: loss improved from 0.11233 to 0.08096, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 7/100\n",
      "116/116 [==============================] - 37s 317ms/step - loss: 0.0561 - accuracy: 0.9769 - val_loss: 0.8345 - val_accuracy: 0.7992\n",
      "\n",
      "Epoch 00007: loss improved from 0.08096 to 0.06318, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 8/100\n",
      "116/116 [==============================] - 40s 341ms/step - loss: 0.0520 - accuracy: 0.9792 - val_loss: 0.9611 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00008: loss improved from 0.06318 to 0.05789, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 9/100\n",
      "116/116 [==============================] - 41s 354ms/step - loss: 0.0436 - accuracy: 0.9811 - val_loss: 1.0124 - val_accuracy: 0.7987\n",
      "\n",
      "Epoch 00009: loss improved from 0.05789 to 0.04149, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 10/100\n",
      "116/116 [==============================] - 40s 342ms/step - loss: 0.0370 - accuracy: 0.9827 - val_loss: 1.1992 - val_accuracy: 0.8036\n",
      "\n",
      "Epoch 00010: loss improved from 0.04149 to 0.03678, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 11/100\n",
      "116/116 [==============================] - 38s 326ms/step - loss: 0.0403 - accuracy: 0.9840 - val_loss: 1.0347 - val_accuracy: 0.8009\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.03678\n",
      "Epoch 12/100\n",
      "116/116 [==============================] - 37s 318ms/step - loss: 0.0251 - accuracy: 0.9886 - val_loss: 1.0815 - val_accuracy: 0.7906\n",
      "\n",
      "Epoch 00012: loss improved from 0.03678 to 0.02704, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 13/100\n",
      "116/116 [==============================] - 37s 321ms/step - loss: 0.0310 - accuracy: 0.9861 - val_loss: 1.3244 - val_accuracy: 0.8041\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.02704\n",
      "Epoch 14/100\n",
      "116/116 [==============================] - 39s 333ms/step - loss: 0.0190 - accuracy: 0.9914 - val_loss: 1.0943 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00014: loss improved from 0.02704 to 0.02613, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 15/100\n",
      "116/116 [==============================] - 40s 341ms/step - loss: 0.0287 - accuracy: 0.9876 - val_loss: 1.3973 - val_accuracy: 0.8030\n",
      "\n",
      "Epoch 00015: loss improved from 0.02613 to 0.02550, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 16/100\n",
      "116/116 [==============================] - 41s 349ms/step - loss: 0.0256 - accuracy: 0.9878 - val_loss: 1.6384 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.02550\n",
      "Epoch 17/100\n",
      "116/116 [==============================] - 40s 343ms/step - loss: 0.0294 - accuracy: 0.9872 - val_loss: 1.9448 - val_accuracy: 0.8160\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.02550\n",
      "Epoch 18/100\n",
      "116/116 [==============================] - 38s 330ms/step - loss: 0.0465 - accuracy: 0.9844 - val_loss: 1.2488 - val_accuracy: 0.7965\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.02550\n",
      "Epoch 19/100\n",
      "116/116 [==============================] - 37s 319ms/step - loss: 0.0182 - accuracy: 0.9922 - val_loss: 1.5358 - val_accuracy: 0.7819\n",
      "\n",
      "Epoch 00019: loss improved from 0.02550 to 0.02269, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 20/100\n",
      "116/116 [==============================] - 37s 316ms/step - loss: 0.0229 - accuracy: 0.9892 - val_loss: 1.3234 - val_accuracy: 0.7917\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.02269\n",
      "Epoch 21/100\n",
      "116/116 [==============================] - 39s 336ms/step - loss: 0.0195 - accuracy: 0.9920 - val_loss: 1.4101 - val_accuracy: 0.8052\n",
      "\n",
      "Epoch 00021: loss improved from 0.02269 to 0.02056, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 22/100\n",
      "116/116 [==============================] - 41s 351ms/step - loss: 0.0161 - accuracy: 0.9930 - val_loss: 1.3053 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00022: loss improved from 0.02056 to 0.01585, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 23/100\n",
      "116/116 [==============================] - 41s 353ms/step - loss: 0.0121 - accuracy: 0.9953 - val_loss: 1.4379 - val_accuracy: 0.8084\n",
      "\n",
      "Epoch 00023: loss improved from 0.01585 to 0.01472, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 24/100\n",
      "116/116 [==============================] - 39s 333ms/step - loss: 0.0141 - accuracy: 0.9935 - val_loss: 1.7661 - val_accuracy: 0.8074\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.01472\n",
      "Epoch 25/100\n",
      "116/116 [==============================] - 37s 319ms/step - loss: 0.0130 - accuracy: 0.9943 - val_loss: 1.8337 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00025: loss improved from 0.01472 to 0.01198, saving model to LSTM_models\\best_LSTM_CNN_model.h5\n",
      "Epoch 26/100\n",
      "116/116 [==============================] - 37s 318ms/step - loss: 0.0092 - accuracy: 0.9964 - val_loss: 1.8541 - val_accuracy: 0.8117\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.01198\n",
      "Epoch 27/100\n",
      "116/116 [==============================] - 38s 329ms/step - loss: 0.0139 - accuracy: 0.9929 - val_loss: 1.7296 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01198\n",
      "Epoch 28/100\n",
      "116/116 [==============================] - 40s 341ms/step - loss: 0.0096 - accuracy: 0.9965 - val_loss: 1.9389 - val_accuracy: 0.7852\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01198\n",
      "Epoch 29/100\n",
      "116/116 [==============================] - 41s 351ms/step - loss: 0.0170 - accuracy: 0.9929 - val_loss: 1.6248 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01198\n",
      "Epoch 30/100\n",
      "116/116 [==============================] - 39s 339ms/step - loss: 0.0235 - accuracy: 0.9916 - val_loss: 1.6004 - val_accuracy: 0.7955\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01198\n",
      "Epoch 31/100\n",
      "116/116 [==============================] - 38s 326ms/step - loss: 0.0135 - accuracy: 0.9947 - val_loss: 1.4161 - val_accuracy: 0.7781\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01198\n",
      "Epoch 32/100\n",
      "116/116 [==============================] - 37s 319ms/step - loss: 0.0298 - accuracy: 0.9899 - val_loss: 1.4517 - val_accuracy: 0.7960\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.01198\n",
      "Epoch 33/100\n",
      "116/116 [==============================] - 37s 323ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 1.7786 - val_accuracy: 0.8068\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.01198\n",
      "Epoch 34/100\n",
      "116/116 [==============================] - 39s 334ms/step - loss: 0.0113 - accuracy: 0.9956 - val_loss: 1.4574 - val_accuracy: 0.7749\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.01198\n",
      "Epoch 35/100\n",
      "116/116 [==============================] - 40s 346ms/step - loss: 0.0127 - accuracy: 0.9940 - val_loss: 1.4850 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.01198\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00035: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f8e85d248>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_BATCH_SIZE = 64\n",
    "DEFAULT_EPOCHS = 100\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "model_cnn.add(SpatialDropout1D(0.2))\n",
    "model_cnn.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model_cnn.add(Conv1D(64, 4))\n",
    "model_cnn.add(GlobalMaxPool1D())\n",
    "model_cnn.add(Dense(64, activation='relu'))\n",
    "model_cnn.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_cnn.compile(optimizer = 'adam', \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop_cnn = keras.callbacks.EarlyStopping(monitor = 'loss',\n",
    "                                                  verbose = 1,\n",
    "                                                  patience = 10,\n",
    "                                                  mode = 'auto',\n",
    "                                                  restore_best_weights = True\n",
    "                                                 )\n",
    "\n",
    "checkpoint_cnn = ModelCheckpoint('LSTM_models/best_LSTM_CNN_model.h5', monitor='loss', mode='auto', \n",
    "                             verbose = 1, save_best_only=True)\n",
    "\n",
    "callbacks_list_cnn = [checkpoint_cnn, early_stop_cnn]\n",
    "\n",
    "model_cnn.fit(x=train_text_vec,\n",
    "              y=y_train_label,\n",
    "              class_weight=cws,\n",
    "              batch_size=DEFAULT_BATCH_SIZE,\n",
    "              epochs=DEFAULT_EPOCHS,\n",
    "              callbacks=callbacks_list_cnn,\n",
    "              verbose=1,\n",
    "              validation_data=(\n",
    "                  test_text_vec,\n",
    "                  y_test_label\n",
    "              ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 53, 128)           1819776   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 53, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 53, 256)           263168    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 50, 64)            65600     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 2,152,834\n",
      "Trainable params: 2,152,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstructed_model_cnn = keras.models.load_model(\"LSTM_models/best_LSTM_CNN_model.h5\")\n",
    "reconstructed_model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[1267  178]\n",
      " [ 196  207]]\n",
      "\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87      1445\n",
      "           1       0.54      0.51      0.53       403\n",
      "\n",
      "    accuracy                           0.80      1848\n",
      "   macro avg       0.70      0.70      0.70      1848\n",
      "weighted avg       0.79      0.80      0.80      1848\n",
      "\n",
      "\n",
      "The evaluation report of classification is:\n",
      "Confusion Matrix:\n",
      "[[1267  178]\n",
      " [ 196  207]]\n",
      "Accuracy: 0.7976190476190477\n",
      "Precision: 0.5376623376623376\n",
      "Recall: 0.5136476426799007\n",
      "F2 Score: 0.5182774161241863\n",
      "AUC Score: 0.8162861583109378\n",
      "\n",
      "{'threshold': 0.01, 'score': 0.6185567010309277, 'y_pred': array([1, 0, 0, ..., 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "model_cnn.load_weights('LSTM_models/best_LSTM_CNN_model.h5')\n",
    "\n",
    "y_test_hat_cnn = model_cnn.predict(test_text_vec)\n",
    "confusion_cnn = confusion_matrix(np.argmax(y_test_label,axis=1), np.argmax(y_test_hat_cnn,axis=1))\n",
    "class_report_cnn = classification_report(np.argmax(y_test_label, axis=1), np.argmax(y_test_hat_cnn, axis=1))\n",
    "                                     \n",
    "print(\"Confusion matrix:\\n\", confusion_cnn)\n",
    "print(\"\\n\")\n",
    "print(\"Classification report:\\n\",class_report_cnn)\n",
    "\n",
    "perf_metrics_cnn = evaluate.performance(y_test, np.argmax(y_test_hat_cnn,axis=1), y_test_hat_cnn)\n",
    "print(perf_metrics_cnn['report'])\n",
    "\n",
    "threshold_metrics_cnn = evaluate.threshold(y_test_hat_cnn, y_test)\n",
    "print(threshold_metrics_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# sns.distplot(y_test_hat_cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/data_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### tf-count and topic model for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The lemmatization does not work really well.\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# df['tweet'] = df['tweet'].apply(lambda x:' '.join(WordNetLemmatizer().lemmatize(i) for i in x.split(' ')))\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Adding a list of stop words to the wordlist\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(['suicidal','bc','did','didn','t','does','doesn','don','dont','doing','going','gonna','having','isn','ll','ve','wanna','want','wanted','wanting','wasn','went','yes','yeah'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=0.01, stop_words=my_stop_words)\n",
    "x = count_vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "n,m = x.shape\n",
    "k = 20 # try for 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=k, random_state=2021)\n",
    "xtr = lda.fit_transform(x)\n",
    "\n",
    "# So xtr will be new features to replace 'tweet'\n",
    "data_x = pd.DataFrame(xtr).add_prefix('topic_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take log for some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_log = ['nlikes', 'nreplies', 'nretweets','join_time', 'tweets', 'following', 'followers', 'likes', 'media']\n",
    "for feature in features_to_log:\n",
    "    data_x[feature] = np.log1p(df[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'day' take absolute values, tweet_length take min-max scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = (df['day'] - 4).abs()\n",
    "df['tweet_length'] = df['tweet_length'] / df['tweet_length'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_add = ['day','url','tweet_length', 'tweet_sentiment', 'bio_sentiment', 'first_person', 'second_person', 'third_person']\n",
    "for feature in features_to_add:\n",
    "    data_x[feature] = df[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data_x, data_y, test_size = 0.2, random_state = 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The evaluation report of OVR is:\n",
      "Confusion Matrix:\n",
      "[[1022  181  254]\n",
      " [ 157  102  164]\n",
      " [  86   66  256]]\n",
      "Accuracy: 0.6031468531468531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "estimator = linear_model.LogisticRegression(class_weight='balanced',\n",
    "                                            multi_class='ovr',\n",
    "                                            C=0.01,\n",
    "                                            max_iter=1000)\n",
    "\n",
    "estimator.fit(x_train, y_train)\n",
    "y_pred = estimator.predict(x_test)\n",
    "report = \"\"\"\n",
    "The evaluation report of OVR is:\n",
    "Confusion Matrix:\n",
    "{}\n",
    "Accuracy: {}\n",
    "\"\"\".format(metrics.confusion_matrix(y_test, y_pred),\n",
    "           metrics.accuracy_score(y_test, y_pred))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
